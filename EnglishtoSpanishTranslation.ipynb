{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charchar1245/Transformer-for-Translating-English-to-Spanish/blob/main/EnglishtoSpanishTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding data for the model -- English Spanish Translation"
      ],
      "metadata": {
        "id": "C-9LBaVPNqT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchtext torchaudio\n",
        "!pip install torch==2.2.2 torchtext==0.17.2 torchaudio==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "Rvpp23OSOGao",
        "outputId": "dd5cc9e4-5c25-4e81-99b4-359b52c5d159"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchtext 0.17.2\n",
            "Uninstalling torchtext-0.17.2:\n",
            "  Successfully uninstalled torchtext-0.17.2\n",
            "Found existing installation: torchaudio 2.2.2\n",
            "Uninstalling torchaudio-2.2.2:\n",
            "  Successfully uninstalled torchaudio-2.2.2\n",
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchtext==0.17.2\n",
            "  Using cached torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torchaudio==2.2.2\n",
            "  Using cached torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.9.86)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchtext, torchaudio\n",
            "Successfully installed torch-2.2.2 torchaudio-2.2.2 torchtext-0.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "97a119d1ba6646a48cf75762c8006506"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y timm fastai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIc8EB79eFxF",
        "outputId": "4cf67d3f-91c7-4238-dab8-544ff0ad52c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping timm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "https://huggingface.co/datasets/okezieowen/english_to_spanish"
      ],
      "metadata": {
        "id": "57nqSDA5QV9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "ds = load_dataset(\"okezieowen/english_to_spanish\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW2Ujag6QBar",
        "outputId": "7df0805d-2ad7-4941-ab29-1ae57b8a9c7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# To see the head of the dataset, you need to access a specific split first.\n",
        "# For example, to see the first 10 elements of the 'train' split:\n",
        "if 'train' in ds:\n",
        "    selected_data = ds['train'].select(range(10))\n",
        "    df = pd.DataFrame(selected_data)\n",
        "    print(f\"Displaying first 10 examples from the 'train' split:\")\n",
        "    display(df)\n",
        "elif len(ds) > 0:\n",
        "    # If 'train' split is not available, try to get the first available split\n",
        "    first_split_name = list(ds.keys())[0]\n",
        "    selected_data = ds[first_split_name].select(range(10))\n",
        "    df = pd.DataFrame(selected_data)\n",
        "    print(f\"Showing first 10 examples from the '{first_split_name}' split:\")\n",
        "    display(df)\n",
        "else:\n",
        "    print(\"DatasetDict is empty or has no accessible splits.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "uBx8331pT97A",
        "outputId": "ff6a08c4-088a-494b-b958-354fe0bbff62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying first 10 examples from the 'train' split:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             English  \\\n",
              "0  I declare resumed the session of the European ...   \n",
              "1  Although, as you will have seen, the dreaded '...   \n",
              "2  You have requested a debate on this subject in...   \n",
              "3  In the meantime, I should like to observe a mi...   \n",
              "4     Please rise, then, for this minute' s silence.   \n",
              "5  (The House rose and observed a minute' s silence)   \n",
              "6              Madam President, on a point of order.   \n",
              "7  You will be aware from the press and televisio...   \n",
              "8  One of the people assassinated very recently i...   \n",
              "9  Would it be appropriate for you, Madam Preside...   \n",
              "\n",
              "                                             Spanish  \n",
              "0  Declaro reanudado el per칤odo de sesiones del P...  \n",
              "1  Como todos han podido comprobar, el gran \"efec...  \n",
              "2  Sus Se침or칤as han solicitado un debate sobre el...  \n",
              "3  A la espera de que se produzca, de acuerdo con...  \n",
              "4  Invito a todos a que nos pongamos de pie para ...  \n",
              "5  (El Parlamento, de pie, guarda un minuto de si...  \n",
              "6  Se침ora Presidenta, una cuesti칩n de procedimiento.  \n",
              "7  Sabr치 usted por la prensa y la televisi칩n que ...  \n",
              "8  Una de las personas que recientemente han ases...  \n",
              "9  쯉er칤a apropiado que usted, Se침ora Presidenta,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0eeeaf3-71a8-4e93-abb3-58a750d63899\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I declare resumed the session of the European ...</td>\n",
              "      <td>Declaro reanudado el per칤odo de sesiones del P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Although, as you will have seen, the dreaded '...</td>\n",
              "      <td>Como todos han podido comprobar, el gran \"efec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You have requested a debate on this subject in...</td>\n",
              "      <td>Sus Se침or칤as han solicitado un debate sobre el...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In the meantime, I should like to observe a mi...</td>\n",
              "      <td>A la espera de que se produzca, de acuerdo con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please rise, then, for this minute' s silence.</td>\n",
              "      <td>Invito a todos a que nos pongamos de pie para ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(The House rose and observed a minute' s silence)</td>\n",
              "      <td>(El Parlamento, de pie, guarda un minuto de si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Madam President, on a point of order.</td>\n",
              "      <td>Se침ora Presidenta, una cuesti칩n de procedimiento.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>You will be aware from the press and televisio...</td>\n",
              "      <td>Sabr치 usted por la prensa y la televisi칩n que ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>One of the people assassinated very recently i...</td>\n",
              "      <td>Una de las personas que recientemente han ases...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Would it be appropriate for you, Madam Preside...</td>\n",
              "      <td>쯉er칤a apropiado que usted, Se침ora Presidenta,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0eeeaf3-71a8-4e93-abb3-58a750d63899')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0eeeaf3-71a8-4e93-abb3-58a750d63899 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0eeeaf3-71a8-4e93-abb3-58a750d63899');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4dda065b-3c3b-4498-bb04-661d5929ca9f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4dda065b-3c3b-4498-bb04-661d5929ca9f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4dda065b-3c3b-4498-bb04-661d5929ca9f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b216c3bf-c417-40d9-94b1-cbaab0f5bd2c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b216c3bf-c417-40d9-94b1-cbaab0f5bd2c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\",\n          \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n          \"(The House rose and observed a minute' s silence)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spanish\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visit\\u00f3 el Parlamento Europeo.\",\n          \"Como todos han podido comprobar, el gran \\\"efecto del a\\u00f1o 2000\\\" no se ha producido. En cambio, los ciudadanos de varios de nuestros pa\\u00edses han sido v\\u00edctimas de cat\\u00e1strofes naturales verdaderamente terribles.\",\n          \"(El Parlamento, de pie, guarda un minuto de silencio)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cd65b44",
        "outputId": "f4e7b591-abf3-43ea-c7a0-4d0789148e9d"
      },
      "source": [
        "# Ensure spaCy English model is loaded if not already\n",
        "# If you encounter an error, uncomment and run the following line:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the tokenization function (if not already defined)\n",
        "def tokenize_en(text):\n",
        "    \"\"\"Tokenizes English text using a spaCy en_core_web_sm model and returns a list of tokens.\"\"\"\n",
        "    # Check if spacy_en is defined globally, otherwise load it\n",
        "    if 'spacy_en' not in globals():\n",
        "        global spacy_en\n",
        "        import spacy\n",
        "        spacy_en = spacy.load('en_core_web_sm')\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Apply tokenization to the 'English' column of the 'train' split\n",
        "# Filter out None values before tokenizing\n",
        "tokenized_english_sentences = [tokenize_en(sentence) for sentence in ds['train']['English'] if sentence is not None]\n",
        "\n",
        "# Display the first 5 tokenized English sentences\n",
        "print(\"First 5 tokenized English sentences:\")\n",
        "for i, tokens in enumerate(tokenized_english_sentences[:5]):\n",
        "    print(tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 tokenized English sentences:\n",
            "['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
            "['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'\", 'millennium', 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
            "['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part', '-', 'session', '.']\n",
            "['In', 'the', 'meantime', ',', 'I', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'Members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'European', 'Union', '.']\n",
            "['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd5f5bcb"
      },
      "source": [
        "First, let's build a vocabulary from our `tokenized_english_sentences`. This vocabulary will map each unique word to a unique integer ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d86cc4",
        "outputId": "6733b3db-a3d2-4c7b-f006-c4ffc2a3ca74"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Flatten the list of tokenized sentences to get all tokens\n",
        "all_tokens = [token for sentence in tokenized_english_sentences for token in sentence]\n",
        "\n",
        "# Build vocabulary with special tokens\n",
        "UNK_IDX = 0 # Unknown token index\n",
        "PAD_IDX = 1 # Padding token index\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    [sentence for sentence in tokenized_english_sentences], # Pass an iterator of token lists\n",
        "    min_freq=1,\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example mapping: 'hello' -> {vocab(['hello'])[0]}, '<pad>' -> {vocab(['<pad>'])[0]}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 37310\n",
            "Example mapping: 'hello' -> 33348, '<pad>' -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a7f6b6"
      },
      "source": [
        "Next, we'll numericalize the tokenized sentences (convert tokens to their vocabulary IDs) and then pad them to a consistent length. We'll choose a maximum sequence length, and either pad shorter sentences or truncate longer ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1c6764",
        "outputId": "cfc7303e-0bb3-4810-cda5-2d0928aa3381"
      },
      "source": [
        "MAX_SEQ_LEN = 100 # Define a maximum sequence length\n",
        "\n",
        "def numericalize_and_pad(sentences, vocab, max_seq_len, pad_idx=PAD_IDX):\n",
        "    numericalized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Add <bos> and <eos> tokens\n",
        "        indexed_sentence = [vocab['<bos>']] + vocab(sentence) + [vocab['<eos>']]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indexed_sentence) < max_seq_len:\n",
        "            padded_sentence = indexed_sentence + [pad_idx] * (max_seq_len - len(indexed_sentence))\n",
        "        else:\n",
        "            padded_sentence = indexed_sentence[:max_seq_len]\n",
        "\n",
        "        numericalized_sentences.append(padded_sentence)\n",
        "    return torch.tensor(numericalized_sentences)\n",
        "\n",
        "# Numericalize and pad the English sentences\n",
        "input_sequences_numerical = numericalize_and_pad(tokenized_english_sentences, vocab, MAX_SEQ_LEN, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of numerical input sequences: {input_sequences_numerical.shape}\")\n",
        "print(\"First numerical input sequence:\", input_sequences_numerical[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of numerical input sequences: torch.Size([149829, 100])\n",
            "First numerical input sequence: tensor([    2,    14,  2756,  2673,     4,  1028,     7,     4,    30,    50,\n",
            "         4451,    16,  2462,  2104,  1083,   587,     5,     9,    14,    36,\n",
            "           62,   367,   261,     8,   307,    52,    12,  1472,    96,   166,\n",
            "           10,     4,   198,    13,    52,  4087,    12,  6453, 18740,   537,\n",
            "            6,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42d7b2b"
      },
      "source": [
        "Now, we define an embedding layer to convert these numerical IDs into dense vector representations. After that, we'll add positional encodings, which are crucial for transformers to understand the order of words in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deb4e618",
        "outputId": "9019b2f1-5239-4872-f62f-e6cfed8c93c4"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "EMBEDDING_DIM = 256 # Dimension of word embeddings\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(len(vocab), EMBEDDING_DIM, padding_idx=PAD_IDX)\n",
        "\n",
        "BATCH_SIZE = 32 # Batch size\n",
        "dataset = TensorDataset(input_sequences_numerical)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Pass the numerical sequences through the embedding layer\n",
        "# input_embeddings = embedding_layer(input_sequences_numerical)\n",
        "for batch_idx, (batch,) in enumerate(dataloader):\n",
        "    # batch: (batch_size, seq_len)\n",
        "\n",
        "    input_embeddings = embedding_layer(batch)\n",
        "    # input_embeddings: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    # 游댳 later: positional encoding, attention, loss, backprop\n",
        "\n",
        "    if batch_idx == 0:\n",
        "        print(input_embeddings.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Shape of input embeddings: {input_embeddings.shape}\") # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "# Define Positional Encoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0., max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        # self.pe: (1, max_len, d_model)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# Instantiate and apply positional encoding\n",
        "positional_encoder = PositionalEncoding(EMBEDDING_DIM, MAX_SEQ_LEN)\n",
        "input_embeddings_with_pos = positional_encoder(input_embeddings)\n",
        "\n",
        "print(f\"Shape of input embeddings with positional encoding: {input_embeddings_with_pos.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Jyx6lSnD2D",
        "outputId": "b13678aa-6a32-4445-b5c7-9635b36a385e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input embeddings: torch.Size([5, 100, 256])\n",
            "Shape of input embeddings with positional encoding: torch.Size([5, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ada4bf8"
      },
      "source": [
        "First, let's define the `MultiHeadSelfAttention` module. This module allows the model to jointly attend to information from different representation subspaces at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af084bc6"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_dim)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        value = value.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        key = key.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        values = self.values(value)\n",
        "        keys = self.keys(key)\n",
        "        queries = self.queries(query)\n",
        "\n",
        "        # Einsum does matrix multiplication for query * key.T\n",
        "        # with shape (N, heads, query_len, head_dim) * (N, heads, head_dim, key_len) -> (N, heads, query_len, key_len)\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# num_heads = 8 # Number of attention heads\n",
        "\n",
        "# value = torch.randn(N, seq_len, embed_dim)\n",
        "# key = torch.randn(N, seq_len, embed_dim)\n",
        "# query = torch.randn(N, seq_len, embed_dim)\n",
        "# mask = torch.ones(N, 1, seq_len, seq_len) # Example mask (no masking for simplicity)\n",
        "\n",
        "# attention_block = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "# output = attention_block(value, key, query, mask)\n",
        "# print(f\"Output shape of MultiHeadSelfAttention: {output.shape}\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1eb8ab"
      },
      "source": [
        "Next, we define a simple `FeedForwardBlock`, which is applied to each position separately and identically. This typically consists of two linear transformations with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93af077"
      },
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, forward_expansion):\n",
        "        super(FeedForwardBlock, self).__init__()\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * forward_expansion),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * forward_expansion, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.feed_forward(x)\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# forward_expansion = 4\n",
        "\n",
        "# x = torch.randn(N, seq_len, embed_dim)\n",
        "\n",
        "# ff_block = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "# output = ff_block(x)\n",
        "# print(f\"Output shape of FeedForwardBlock: {output.shape}\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5b2594"
      },
      "source": [
        "Finally, we combine these two components into an `EncoderBlock`. This block integrates multi-head self-attention, a feed-forward network, residual connections, and layer normalization, forming a fundamental building block of a Transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196da9c0"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-head self-attention with residual connection and layer normalization\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention + x))\n",
        "\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(feed_forward + x))\n",
        "        return out\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# num_heads = 8 # Number of attention heads\n",
        "# forward_expansion = 4 # Expansion factor for feed-forward layer\n",
        "# dropout = 0.1 # Dropout rate\n",
        "\n",
        "# x = torch.randn(N, seq_len, embed_dim)\n",
        "# mask = torch.ones(N, 1, seq_len, seq_len) # Example mask\n",
        "\n",
        "# encoder_block = EncoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "# output = encoder_block(x, mask)\n",
        "# print(f\"Output shape of EncoderBlock: {output.shape}\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebcdeac9"
      },
      "source": [
        "Let's test the `EncoderBlock` with the `input_embeddings_with_pos` we generated earlier. Note that `input_embeddings_with_pos` had a batch size of 5 from the previous `dataloader` iteration, not the full `BATCH_SIZE` of 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f7d023",
        "outputId": "06c1c188-c5e4-4c55-935c-0ee2ce15739d"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Redefine the batch size to match what was in input_embeddings_with_pos\n",
        "# The previous run only processed 5 items in the last batch if the dataset size was not divisible by BATCH_SIZE\n",
        "# Or, to be safe, let's just make a dummy input with the current batch size used in input_embeddings_with_pos\n",
        "current_batch_size = input_embeddings_with_pos.shape[0]\n",
        "\n",
        "# Assuming input_embeddings_with_pos comes from the previous run with a certain batch size\n",
        "# We will use this size for our dummy input to test the EncoderBlock\n",
        "\n",
        "# Set parameters for the EncoderBlock\n",
        "EMBEDDING_DIM = input_embeddings_with_pos.shape[2] # Use the actual embedding dimension\n",
        "NUM_HEADS = 8  # A common choice, adjust as needed\n",
        "FORWARD_EXPANSION = 4 # A common choice, adjust as needed\n",
        "DROPOUT = 0.1 # A common choice, adjust as needed\n",
        "\n",
        "# Create a dummy mask for testing. For an encoder, this is typically all ones (no masking within sequence).\n",
        "# Or it can be used for padding mask, where padded tokens are masked out.\n",
        "# Here we assume no padding mask for simplicity in this test.\n",
        "mask = torch.ones(current_batch_size, 1, MAX_SEQ_LEN, MAX_SEQ_LEN)\n",
        "\n",
        "# Instantiate the EncoderBlock\n",
        "encoder_block = EncoderBlock(EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT)\n",
        "\n",
        "# Pass the input embeddings with positional encoding through the EncoderBlock\n",
        "output_encoder_block = encoder_block(input_embeddings_with_pos, mask)\n",
        "\n",
        "print(f\"Shape of output after EncoderBlock: {output_encoder_block.shape}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output after EncoderBlock: torch.Size([5, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8598b10"
      },
      "source": [
        "# Task\n",
        "Tokenize the Spanish sentences in the dataset using spaCy's `es_core_news_sm` model, then build a Spanish vocabulary, and numericalize and pad the tokenized Spanish sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6565cb2"
      },
      "source": [
        "## Tokenize Spanish Sentences\n",
        "\n",
        "### Subtask:\n",
        "Define a tokenization function for Spanish text using spaCy's es_core_news_sm model and apply it to the Spanish column of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b36f17"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a tokenization function for Spanish text using spaCy and apply it to the 'Spanish' column of the dataset. This involves importing spaCy, defining the `tokenize_es` function to handle model loading and tokenization, and then applying this function to the relevant dataset column while handling potential `None` values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2ddf8d",
        "outputId": "76de9bca-9efd-4703-dddd-60a19d3a2d46"
      },
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Define the tokenization function for Spanish\n",
        "def tokenize_es(text):\n",
        "    \"\"\"Tokenizes Spanish text using a spaCy es_core_news_sm model and returns a list of tokens.\"\"\"\n",
        "    # Check if spacy_es is defined globally, otherwise load it\n",
        "    if 'spacy_es' not in globals():\n",
        "        global spacy_es\n",
        "        spacy_es = spacy.load('es_core_news_sm')\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "# Apply tokenization to the 'Spanish' column of the 'train' split\n",
        "# Filter out None values before tokenizing\n",
        "tokenized_spanish_sentences = [tokenize_es(sentence) for sentence in ds['train']['Spanish'] if sentence is not None]\n",
        "\n",
        "# Display the first 5 tokenized Spanish sentences\n",
        "print(\"First 5 tokenized Spanish sentences:\")\n",
        "for i, tokens in enumerate(tokenized_spanish_sentences[:5]):\n",
        "    print(tokens)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/compat.py\", line 5, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m九 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m丘 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "First 5 tokenized Spanish sentences:\n",
            "['Declaro', 'reanudado', 'el', 'per칤odo', 'de', 'sesiones', 'del', 'Parlamento', 'Europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'Sus', 'Se침or칤as', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.']\n",
            "['Como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '\"', 'efecto', 'del', 'a침o', '2000', '\"', 'no', 'se', 'ha', 'producido', '.', 'En', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'pa칤ses', 'han', 'sido', 'v칤ctimas', 'de', 'cat치strofes', 'naturales', 'verdaderamente', 'terribles', '.']\n",
            "['Sus', 'Se침or칤as', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'pr칩ximos', 'd칤as', ',', 'en', 'el', 'curso', 'de', 'este', 'per칤odo', 'de', 'sesiones', '.']\n",
            "['A', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'v칤ctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'pa칤ses', 'de', 'la', 'Uni칩n', 'Europea', 'afectados', '.']\n",
            "['Invito', 'a', 'todos', 'a', 'que', 'nos', 'pongamos', 'de', 'pie', 'para', 'guardar', 'un', 'minuto', 'de', 'silencio', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8ebc99"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully tokenized the Spanish sentences. Now I need to build a vocabulary from these tokenized Spanish sentences, similar to how the English vocabulary was built. This is the next logical step in preparing the Spanish data for the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b9dc0e3",
        "outputId": "99e3c037-fa7c-4ae3-da0d-19cb17f2b47f"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Build vocabulary for Spanish with special tokens\n",
        "# UNK_IDX and PAD_IDX are already defined globally from the English vocabulary creation\n",
        "# UNK_IDX = 0\n",
        "# PAD_IDX = 1\n",
        "\n",
        "vocab_es = build_vocab_from_iterator(\n",
        "    [sentence for sentence in tokenized_spanish_sentences], # Pass an iterator of token lists\n",
        "    min_freq=1,\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab_es.set_default_index(UNK_IDX)\n",
        "\n",
        "print(f\"Spanish Vocabulary size: {len(vocab_es)}\")\n",
        "print(f\"Example mapping: 'amigo' -> {vocab_es(['amigo'])[0]}, '<pad>' -> {vocab_es(['<pad>'])[0]}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spanish Vocabulary size: 66909\n",
            "Example mapping: 'amigo' -> 3052, '<pad>' -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180bd507"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Spanish vocabulary has been built, the next step is to numericalize the tokenized Spanish sentences (convert tokens to their vocabulary IDs) and then pad them to a consistent length, similar to how the English sentences were processed. This will prepare the Spanish data for model input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86e07c14",
        "outputId": "01dfb7d0-0f72-4ef8-b012-d7a3623038bc"
      },
      "source": [
        "MAX_SEQ_LEN = 100 # Ensure MAX_SEQ_LEN is defined or accessible\n",
        "\n",
        "# Numericalize and pad the Spanish sentences\n",
        "output_sequences_numerical = numericalize_and_pad(tokenized_spanish_sentences, vocab_es, MAX_SEQ_LEN, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of numerical output sequences: {output_sequences_numerical.shape}\")\n",
        "print(\"First numerical output sequence:\", output_sequences_numerical[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of numerical output sequences: torch.Size([149559, 100])\n",
            "First numerical output sequence: tensor([   2, 5649, 8464,   10,  572,    4, 1219,   16,   37,   82,    5, 5005,\n",
            "          10, 2013, 2062,    4, 1082,  280,    5,   11, 9664,   12, 2202,  273,\n",
            "          75,  607,    4,    7,  830,  491,  434, 1362, 5570,    8,    3,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b603cf1"
      },
      "source": [
        "## Define Decoder Block\n",
        "\n",
        "### Subtask:\n",
        "Define the `DecoderBlock` module. This block will incorporate masked multi-head self-attention (for the target sequence), multi-head attention over the encoder's output, a feed-forward network, residual connections, and layer normalization, following the standard Transformer architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a49f5ea"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `DecoderBlock` class as instructed, incorporating masked multi-head self-attention, multi-head attention over the encoder's output, a feed-forward network, residual connections, layer normalization, and dropout layers, following the provided structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "361dd098",
        "outputId": "e5eb0f2c-75ca-446a-a6d2-cfc15bb6bdcf"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Masked Multi-Head Self-Attention for the target sequence\n",
        "        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # Multi-Head Attention over the encoder's output\n",
        "        self.encoder_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, trg_mask, src_mask):\n",
        "        # Masked Multi-Head Self-Attention\n",
        "        # Query, Key, Value are all from the target sequence (x)\n",
        "        self_attention = self.self_attention(x, x, x, trg_mask)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        x = self.dropout1(self.norm1(self_attention + x))\n",
        "\n",
        "        # Multi-Head Encoder-Decoder Attention\n",
        "        # Query from target sequence (x), Key and Value from encoder output (enc_output)\n",
        "        encoder_attention = self.encoder_attention(enc_output, enc_output, x, src_mask)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        x = self.dropout2(self.norm2(encoder_attention + x))\n",
        "\n",
        "        # Feed-forward network\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        out = self.dropout3(self.norm3(feed_forward + x))\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"DecoderBlock class defined.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderBlock class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b282c71"
      },
      "source": [
        "**Reasoning**:\n",
        "The `DecoderBlock` class has been defined. Now, I need to test its functionality by creating a dummy input, `enc_output`, and the necessary masks, then instantiating the `DecoderBlock` and passing these through its `forward` method. This will verify that the block can process inputs and produce an output with the expected shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e3319e",
        "outputId": "b44348e7-07d2-4b47-d8e4-7b9e4239dc75"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Assuming embed_dim, num_heads, forward_expansion, dropout are already defined globally\n",
        "# EMBEDDING_DIM = 256\n",
        "# NUM_HEADS = 8\n",
        "# FORWARD_EXPANSION = 4\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "# Example Usage:\n",
        "N = 2 # Batch size\n",
        "seq_len_trg = 15 # Target sequence length (for x)\n",
        "seq_len_src = MAX_SEQ_LEN # Source sequence length (for enc_output, from MAX_SEQ_LEN)\n",
        "\n",
        "# Dummy input for the decoder (target embeddings)\n",
        "x = torch.randn(N, seq_len_trg, EMBEDDING_DIM)\n",
        "\n",
        "# Dummy encoder output\n",
        "enc_output = torch.randn(N, seq_len_src, EMBEDDING_DIM)\n",
        "\n",
        "# Dummy target mask (for masked self-attention in decoder)\n",
        "# This mask typically ensures that a token cannot attend to future tokens.\n",
        "# For simplicity, let's use a causal mask. (seq_len_trg, seq_len_trg)\n",
        "trg_mask = torch.tril(torch.ones(seq_len_trg, seq_len_trg)).unsqueeze(0).unsqueeze(0).expand(N, NUM_HEADS, seq_len_trg, seq_len_trg)\n",
        "\n",
        "# Dummy source mask (for encoder-decoder attention)\n",
        "# This mask typically handles padding in the encoder output. Here, assuming no padding mask needed.\n",
        "# For simplicity, we can use an all-ones mask if no actual padding mask is being generated.\n",
        "src_mask = torch.ones(N, 1, 1, seq_len_src) # N, 1, 1, src_seq_len for broadcasting\n",
        "\n",
        "# Instantiate the DecoderBlock\n",
        "decoder_block = DecoderBlock(EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT)\n",
        "\n",
        "# Pass the inputs through the DecoderBlock\n",
        "output_decoder_block = decoder_block(x, enc_output, trg_mask, src_mask)\n",
        "\n",
        "print(f\"Shape of output after DecoderBlock: {output_decoder_block.shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output after DecoderBlock: torch.Size([2, 15, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b90f172"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the successful implementation and testing of the Decoder Block and prepare for integrating it into a full Transformer model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e23254a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Spanish sentence tokenization was successfully performed using spaCy's `es_core_news_sm` model, processing 149,559 sentences.\n",
        "*   A Spanish vocabulary was constructed, comprising 66,909 unique tokens, including special tokens like `<unk>`, `<pad>`, `<bos>`, and `<eos>`.\n",
        "*   The tokenized Spanish sentences were numericalized and padded to a uniform length of 100, resulting in a tensor of shape `(149559, 100)`.\n",
        "*   The `DecoderBlock` module was successfully defined, incorporating key Transformer components: masked multi-head self-attention, encoder-decoder attention, a feed-forward network, residual connections, and layer normalization.\n",
        "*   The functionality of the `DecoderBlock` was validated with dummy inputs, producing an output of shape `(2, 15, 256)`, which aligns with expected dimensions for a batch size of 2, a target sequence length of 15, and an embedding dimension of 256.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The successfully tokenized, vocabulary-built, numericalized, and padded Spanish dataset is now ready for use as target sequences in a sequence-to-sequence model.\n",
        "*   The validated `DecoderBlock` can now be integrated into a complete Transformer Decoder, and subsequently into a full Transformer model for machine translation or other sequence generation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0975ea69",
        "outputId": "2cb29e9b-925c-43e3-e674-8f42775d4db8"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create the embedding layer for Spanish vocabulary\n",
        "# vocab_es is the Spanish vocabulary built earlier\n",
        "# EMBEDDING_DIM and PAD_IDX are already defined globally\n",
        "embedding_layer_es = nn.Embedding(len(vocab_es), EMBEDDING_DIM, padding_idx=PAD_IDX)\n",
        "\n",
        "# Create a DataLoader for the numericalized Spanish sequences\n",
        "# BATCH_SIZE is already defined globally from English data processing\n",
        "dataset_es = TensorDataset(output_sequences_numerical)\n",
        "dataloader_es = DataLoader(dataset_es, batch_size=BATCH_SIZE, shuffle=False) # No need to shuffle for just embedding\n",
        "\n",
        "# --- MODIFIED: Process only the first batch for demonstration ---\n",
        "# In a full training loop, you would iterate through dataloader_es,\n",
        "# embed each batch, and then pass it to the model without concatenating all batches.\n",
        "\n",
        "# Get one batch from the dataloader\n",
        "# Check if dataloader_es is not empty before attempting to get a batch\n",
        "if len(dataloader_es) > 0:\n",
        "    first_batch_numerical_es = next(iter(dataloader_es))[0] # Get the first batch (tensor) from the tuple\n",
        "    # Embed this single batch\n",
        "    sample_output_embeddings = embedding_layer_es(first_batch_numerical_es)\n",
        "    print(f\"Shape of sample raw Spanish embeddings (first batch): {sample_output_embeddings.shape}\")\n",
        "else:\n",
        "    print(\"Dataloader for Spanish sequences is empty.\")\n",
        "    sample_output_embeddings = torch.empty(0)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sample raw Spanish embeddings (first batch): torch.Size([32, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "927258ea",
        "outputId": "4eaad40b-a928-48f8-f760-8d8481a4d523"
      },
      "source": [
        "# Apply the previously defined positional encoder to the Spanish embeddings\n",
        "# positional_encoder was defined for English embeddings and can be reused\n",
        "\n",
        "# Ensure sample_output_embeddings exists and is not empty before processing\n",
        "if sample_output_embeddings.nelement() > 0:\n",
        "    sample_output_embeddings_with_pos = positional_encoder(sample_output_embeddings)\n",
        "    print(f\"Shape of sample Spanish embeddings with positional encoding (first batch): {sample_output_embeddings_with_pos.shape}\")\n",
        "else:\n",
        "    print(\"No sample Spanish embeddings to apply positional encoding to.\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sample Spanish embeddings with positional encoding (first batch): torch.Size([32, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e854a15"
      },
      "source": [
        "# Task\n",
        "Define the `make_src_mask` function to create a padding mask for the encoder's source input. This mask will prevent attention to padding tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fc3fa5b"
      },
      "source": [
        "## Define Source Mask Function\n",
        "\n",
        "### Subtask:\n",
        "Define the `make_src_mask` function to create a padding mask for the encoder's source input. This mask will prevent attention to padding tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b16901a1"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `make_src_mask` function as described, which will create a padding mask for the encoder's source input to prevent attention to padding tokens. This function will take the source tensor, identify padding tokens using `PAD_IDX`, and reshape the resulting boolean mask for compatibility with multi-head attention.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5387576f",
        "outputId": "fe7d5d57-714c-4b6c-db79-b2fcd3990df1"
      },
      "source": [
        "def make_src_mask(src, pad_idx=PAD_IDX):\n",
        "    \"\"\"\n",
        "    Creates a padding mask for the encoder's source input.\n",
        "    This mask prevents attention to padding tokens.\n",
        "    \"\"\"\n",
        "    # src shape: (batch_size, src_len)\n",
        "\n",
        "    # Create a boolean mask: True where src is not PAD_IDX, False where it is.\n",
        "    src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    # src_mask shape: (batch_size, 1, 1, src_len)\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "print(\"make_src_mask function defined.\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make_src_mask function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874bdb0d"
      },
      "source": [
        "**Reasoning**:\n",
        "The `make_src_mask` function has been defined. Now, I need to test its functionality by creating a dummy source tensor and passing it to the function to verify that it generates a mask with the correct shape and values, thus confirming it prevents attention to padding tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4026961",
        "outputId": "1665c2bf-2a6a-4c1d-f232-203518b8f667"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Dummy source tensor for testing (batch_size, src_len)\n",
        "# Let's create a tensor with some padding tokens (represented by PAD_IDX)\n",
        "BATCH_SIZE_TEST = 3 # Example batch size\n",
        "SRC_LEN_TEST = 10   # Example source sequence length\n",
        "\n",
        "# Create a dummy source input with some padding (PAD_IDX=1)\n",
        "# Example: [word, word, PAD, word, PAD, PAD, word, word, word, PAD]\n",
        "dummy_src_input = torch.tensor([\n",
        "    [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX],\n",
        "    [3, 6, 9, 12, PAD_IDX, 15, PAD_IDX, 18, 21, 24],\n",
        "    [4, PAD_IDX, 8, 12, 16, 20, 24, 28, 32, 36]\n",
        "], dtype=torch.long)\n",
        "\n",
        "\n",
        "# Generate the source mask using the defined function\n",
        "source_mask = make_src_mask(dummy_src_input, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of dummy source input: {dummy_src_input.shape}\")\n",
        "print(f\"Shape of generated source mask: {source_mask.shape}\")\n",
        "print(\"Generated source mask (first batch, first head, first query pos):\\n\", source_mask[0, 0, 0, :])\n",
        "\n",
        "# Verify a few elements to ensure correctness\n",
        "# For dummy_src_input[0]:\n",
        "# Expected mask for [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX]\n",
        "# Should be: [True, True, False, True, False, False, True, True, True, False]\n",
        "expected_mask_sample = torch.tensor([True, True, False, True, False, False, True, True, True, False])\n",
        "assert torch.equal(source_mask[0, 0, 0, :], expected_mask_sample), \"Source mask values are incorrect!\"\n",
        "\n",
        "print(\"Source mask generated correctly for the dummy input.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy source input: torch.Size([3, 10])\n",
            "Shape of generated source mask: torch.Size([3, 1, 1, 10])\n",
            "Generated source mask (first batch, first head, first query pos):\n",
            " tensor([ True,  True, False,  True, False, False,  True,  True,  True, False])\n",
            "Source mask generated correctly for the dummy input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29b81603"
      },
      "source": [
        "## Define Target Mask Function\n",
        "\n",
        "### Subtask:\n",
        "Define the `make_trg_mask` function to create a combined padding and causal (look-ahead) mask for the decoder's target input. This mask ensures that attention only considers previous tokens and ignores padding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bff8b81b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `make_trg_mask` function to create a combined padding and causal mask for the decoder's target input, following the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5514895",
        "outputId": "5b88d9d1-69d0-446b-c3a6-ee72e44d4d32"
      },
      "source": [
        "import torch\n",
        "\n",
        "def make_trg_mask(trg, pad_idx=PAD_IDX):\n",
        "    \"\"\"\n",
        "    Creates a combined padding and causal (look-ahead) mask for the decoder's target input.\n",
        "    This mask ensures that attention only considers previous tokens and ignores padding.\n",
        "    \"\"\"\n",
        "    # trg shape: (batch_size, trg_len)\n",
        "\n",
        "    # 1. Create padding mask\n",
        "    # trg_pad_mask shape: (batch_size, 1, 1, trg_len)\n",
        "    trg_pad_mask = (trg != pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    # 2. Create causal (look-ahead) mask\n",
        "    trg_len = trg.shape[1]\n",
        "    # trg_sub_mask shape: (1, 1, trg_len, trg_len)\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "    trg_sub_mask = trg_sub_mask.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # 3. Combine padding mask and causal mask\n",
        "    # The combined mask will be True where both conditions are met (not padding AND not future token)\n",
        "    # The `&` operator performs element-wise logical AND.\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "print(\"make_trg_mask function defined.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make_trg_mask function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfefef5d"
      },
      "source": [
        "**Reasoning**:\n",
        "The `make_trg_mask` function has been defined. Now, I need to test its functionality by creating a dummy target tensor and passing it to the function to verify that it generates a mask with the correct shape and values, thus confirming it prevents attention to future tokens and padding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "44ce6e45",
        "outputId": "3a905322-7ef3-4a78-ed17-38d7a9237509"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Dummy target tensor for testing (batch_size, trg_len)\n",
        "# Let's create a tensor with some padding tokens (represented by PAD_IDX)\n",
        "BATCH_SIZE_TEST = 3 # Example batch size\n",
        "TRG_LEN_TEST = 10   # Example target sequence length\n",
        "\n",
        "# Create a dummy target input with some padding (PAD_IDX=1)\n",
        "# Example: [word, word, PAD, word, PAD, PAD, word, word, word, PAD]\n",
        "dummy_trg_input = torch.tensor([\n",
        "    [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX],\n",
        "    [3, 6, 9, 12, PAD_IDX, 15, PAD_IDX, 18, 21, 24],\n",
        "    [4, PAD_IDX, 8, 12, 16, 20, 24, 28, 32, 36]\n",
        "], dtype=torch.long)\n",
        "\n",
        "# Generate the target mask using the defined function\n",
        "target_mask = make_trg_mask(dummy_trg_input, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of dummy target input: {dummy_trg_input.shape}\")\n",
        "print(f\"Shape of generated target mask: {target_mask.shape}\")\n",
        "print(\"Generated target mask (first batch, first head, first query pos):\\n\", target_mask[0, 0, 0, :])\n",
        "print(\"Generated target mask (first batch, first head, full mask):\\n\", target_mask[0, 0, :, :])\n",
        "\n",
        "# Verify a few elements to ensure correctness\n",
        "# For dummy_trg_input[0] = [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX]\n",
        "# Expected combined mask for first query position (index 0) should be: True, False, False, False, ...\n",
        "# (Only the token itself is visible)\n",
        "\n",
        "# Let's check the first row of the first batch mask for correctness based on padding and causality\n",
        "# Expected mask for the first token at index 0 should be only True for itself and False for others.\n",
        "# Example: Query at index 0 (token 2)\n",
        "#   - Pad mask for token 0 is True\n",
        "#   - Causal mask for token 0 is True for index 0, False for others\n",
        "#   Combined should be: [True, False, False, False, False, False, False, False, False, False]\n",
        "expected_mask_row_0 = torch.tensor([True, False, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 0, :], expected_mask_row_0), \"Target mask row 0 values are incorrect!\"\n",
        "\n",
        "# Example: Query at index 1 (token 5)\n",
        "#   - Pad mask for tokens [2, 5, PAD, 7, PAD, PAD, 8, 9, 10, PAD] => [T, T, F, T, F, F, T, T, T, F]\n",
        "#   - Causal mask for token 1 is [T, T, F, F, F, F, F, F, F, F]\n",
        "#   Combined should be: [True, True, False, False, False, False, False, False, False, False]\n",
        "expected_mask_row_1 = torch.tensor([True, True, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 1, :], expected_mask_row_1), \"Target mask row 1 values are incorrect!\"\n",
        "\n",
        "# Example: Query at index 2 (PAD_IDX)\n",
        "#   - Pad mask for tokens at index 2 is False (due to PAD_IDX)\n",
        "#   - Combined should be all False\n",
        "expected_mask_row_2 = torch.tensor([False, False, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 2, :], expected_mask_row_2), \"Target mask row 2 values are incorrect!\"\n",
        "\n",
        "print(\"Target mask generated correctly for the dummy input.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy target input: torch.Size([3, 10])\n",
            "Shape of generated target mask: torch.Size([3, 1, 10, 10])\n",
            "Generated target mask (first batch, first head, first query pos):\n",
            " tensor([ True, False, False, False, False, False, False, False, False, False])\n",
            "Generated target mask (first batch, first head, full mask):\n",
            " tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True, False, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True,  True, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True,  True,  True, False],\n",
            "        [ True,  True, False,  True, False, False,  True,  True,  True, False]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Target mask row 2 values are incorrect!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-188377180.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m#   - Combined should be all False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mexpected_mask_row_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_mask_row_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Target mask row 2 values are incorrect!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target mask generated correctly for the dummy input.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Target mask row 2 values are incorrect!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d195552a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous test for `make_trg_mask` failed because the padding mask was not correctly applied to both the query and key dimensions, leading to an incorrect mask when the query position itself was a padding token. I need to modify the `make_trg_mask` function to ensure that the padding mask accounts for both query and key padding, and then re-run the test with the corrected function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "234333eb",
        "outputId": "a2dba5f4-fc98-4ad4-ed80-4c6382388a92"
      },
      "source": [
        "import torch\n",
        "\n",
        "def make_trg_mask(trg, pad_idx=PAD_IDX):\n",
        "    \"\"\"\n",
        "    Creates a combined padding and causal (look-ahead) mask for the decoder's target input.\n",
        "    This mask ensures that attention only considers previous tokens and ignores padding.\n",
        "    \"\"\"\n",
        "    # trg shape: (batch_size, trg_len)\n",
        "\n",
        "    # 1. Create padding mask (for both query and key dimensions)\n",
        "    # This creates a boolean tensor of shape (batch_size, trg_len)\n",
        "    # where True means not a padding token.\n",
        "    pad_mask = (trg != pad_idx)\n",
        "\n",
        "    # Expand the mask to cover both query and key dimensions\n",
        "    # query_mask: (batch_size, trg_len, 1) - True if query token is not padding\n",
        "    # key_mask: (batch_size, 1, trg_len) - True if key token is not padding\n",
        "    # The element-wise AND will result in (batch_size, trg_len, trg_len)\n",
        "    # which is True only if BOTH query and key tokens are not padding.\n",
        "    trg_pad_mask = pad_mask.unsqueeze(2) & pad_mask.unsqueeze(1)\n",
        "    # trg_pad_mask shape: (batch_size, trg_len, trg_len)\n",
        "\n",
        "    # 2. Create causal (look-ahead) mask\n",
        "    trg_len = trg.shape[1]\n",
        "    # trg_sub_mask shape: (trg_len, trg_len)\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
        "\n",
        "    # 3. Combine padding mask and causal mask\n",
        "    # The combined mask will be True where both conditions are met (not padding AND not future token)\n",
        "    # We need to unsqueeze trg_pad_mask for the head dimension for broadcasting.\n",
        "    # trg_sub_mask is already (trg_len, trg_len) and will be broadcasted with batch.\n",
        "    # Final target_mask shape: (batch_size, 1, trg_len, trg_len)\n",
        "    trg_mask = trg_pad_mask.unsqueeze(1) & trg_sub_mask\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "print(\"make_trg_mask function defined (fixed).\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make_trg_mask function defined (fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96af418a"
      },
      "source": [
        "**Reasoning**:\n",
        "The `make_trg_mask` function has been fixed. I need to re-run the test to verify its functionality by creating a dummy target tensor and passing it to the function to confirm that it now generates a mask with the correct shape and values, preventing attention to future tokens and padding as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dff2dce",
        "outputId": "a8462f6d-7d6d-46a9-fc98-868613b38da7"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Dummy target tensor for testing (batch_size, trg_len)\n",
        "# Let's create a tensor with some padding tokens (represented by PAD_IDX)\n",
        "BATCH_SIZE_TEST = 3 # Example batch size\n",
        "TRG_LEN_TEST = 10   # Example target sequence length\n",
        "\n",
        "# Create a dummy target input with some padding (PAD_IDX=1)\n",
        "# Example: [word, word, PAD, word, PAD, PAD, word, word, word, PAD]\n",
        "dummy_trg_input = torch.tensor(\n",
        "    [\n",
        "        [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX],\n",
        "        [3, 6, 9, 12, PAD_IDX, 15, PAD_IDX, 18, 21, 24],\n",
        "        [4, PAD_IDX, 8, 12, 16, 20, 24, 28, 32, 36],\n",
        "    ],\n",
        "    dtype=torch.long,\n",
        ")\n",
        "\n",
        "# Generate the target mask using the defined function\n",
        "target_mask = make_trg_mask(dummy_trg_input, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of dummy target input: {dummy_trg_input.shape}\")\n",
        "print(f\"Shape of generated target mask: {target_mask.shape}\")\n",
        "print(\"Generated target mask (first batch, first head, first query pos):\\n\", target_mask[0, 0, 0, :])\n",
        "print(\"Generated target mask (first batch, first head, full mask):\\n\", target_mask[0, 0, :, :])\n",
        "\n",
        "# Verify a few elements to ensure correctness\n",
        "# For dummy_trg_input[0] = [2, 5, PAD_IDX, 7, PAD_IDX, PAD_IDX, 8, 9, 10, PAD_IDX]\n",
        "\n",
        "# Let's check the first row of the first batch mask for correctness based on padding and causality\n",
        "# Example: Query at index 0 (token 2)\n",
        "#   - Pad mask for token 0 is True\n",
        "#   - Causal mask for token 0 is True for index 0, False for others\n",
        "#   Combined should be: [True, False, False, False, False, False, False, False, False, False]\n",
        "expected_mask_row_0 = torch.tensor([True, False, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 0, :], expected_mask_row_0), \"Target mask row 0 values are incorrect!\"\n",
        "\n",
        "# Example: Query at index 1 (token 5)\n",
        "#   - Pad mask for tokens [2, 5, PAD, 7, PAD, PAD, 8, 9, 10, PAD] => [T, T, F, T, F, F, T, T, T, F]\n",
        "#   - Causal mask for token 1 is [T, T, F, F, F, F, F, F, F, F]\n",
        "#   Combined should be: [True, True, False, False, False, False, False, False, False, False]\n",
        "expected_mask_row_1 = torch.tensor([True, True, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 1, :], expected_mask_row_1), \"Target mask row 1 values are incorrect!\"\n",
        "\n",
        "# Example: Query at index 2 (PAD_IDX)\n",
        "#   - This query position itself is a padding token, so the entire row should be False.\n",
        "expected_mask_row_2 = torch.tensor([False, False, False, False, False, False, False, False, False, False])\n",
        "assert torch.equal(target_mask[0, 0, 2, :], expected_mask_row_2), \"Target mask row 2 values are incorrect!\"\n",
        "\n",
        "print(\"Target mask generated correctly for the dummy input (fixed).\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy target input: torch.Size([3, 10])\n",
            "Shape of generated target mask: torch.Size([3, 1, 10, 10])\n",
            "Generated target mask (first batch, first head, first query pos):\n",
            " tensor([ True, False, False, False, False, False, False, False, False, False])\n",
            "Generated target mask (first batch, first head, full mask):\n",
            " tensor([[ True, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False, False, False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True, False, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True,  True, False, False],\n",
            "        [ True,  True, False,  True, False, False,  True,  True,  True, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False]])\n",
            "Target mask generated correctly for the dummy input (fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e07c04f5"
      },
      "source": [
        "## Implement Transformer Class\n",
        "\n",
        "### Subtask:\n",
        "Implement the `Transformer` class, integrating the `EncoderBlock`, `DecoderBlock`, embedding layers for source and target languages, and positional encodings. The class will also include a final linear layer to project the decoder output to the target vocabulary size. The `forward` method will orchestrate the data flow through the entire model, using the `make_src_mask` and `make_trg_mask` functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79f9741d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `Transformer` class as instructed, which integrates all previously defined components and orchestrates the data flow within the model, including embedding layers, positional encodings, encoder and decoder blocks, and the final linear layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24d4ed0a",
        "outputId": "2a3cc6f3-16d3-4f02-89fe-d583563f44cd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming MultiHeadSelfAttention, FeedForwardBlock, EncoderBlock, DecoderBlock, PositionalEncoding,\n",
        "# make_src_mask, make_trg_mask, vocab, vocab_es, EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT, PAD_IDX\n",
        "# are already defined or accessible in the global scope.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        embed_dim,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        max_len=5000\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Source and Target embedding layers\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_dim, padding_idx=src_pad_idx)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embed_dim, padding_idx=trg_pad_idx)\n",
        "\n",
        "        # Positional encoding layer\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, max_len=max_len)\n",
        "\n",
        "        # Dropout layer for the output of positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder stack\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [\n",
        "                EncoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Decoder stack\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Final linear layer to project decoder output to target vocabulary size\n",
        "        self.fc_out = nn.Linear(embed_dim, trg_vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        # trg shape: (batch_size, trg_len)\n",
        "\n",
        "        # Generate masks for source and target sequences\n",
        "        src_mask = make_src_mask(src, src_pad_idx)\n",
        "        trg_mask = make_trg_mask(trg, trg_pad_idx)\n",
        "\n",
        "        # Apply source and target word embeddings\n",
        "        src_embeddings = self.src_word_embedding(src)\n",
        "        trg_embeddings = self.trg_word_embedding(trg)\n",
        "\n",
        "        # Apply positional encoding and dropout to embeddings\n",
        "        src_with_pos = self.dropout(self.positional_encoder(src_embeddings))\n",
        "        trg_with_pos = self.dropout(self.positional_encoder(trg_embeddings))\n",
        "\n",
        "        # Pass through encoder stack\n",
        "        encoder_output = src_with_pos\n",
        "        for layer in self.encoder_blocks:\n",
        "            encoder_output = layer(encoder_output, src_mask)\n",
        "\n",
        "        # Pass through decoder stack\n",
        "        decoder_output = trg_with_pos\n",
        "        for layer in self.decoder_blocks:\n",
        "            decoder_output = layer(decoder_output, encoder_output, trg_mask, src_mask)\n",
        "\n",
        "        # Apply final linear layer\n",
        "        final_output = self.fc_out(decoder_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "print(\"Transformer class defined.\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5538c89"
      },
      "source": [
        "**Reasoning**:\n",
        "The `Transformer` class has been defined. Now, I need to test its functionality by creating dummy input data, masks, and then instantiating the class and passing the dummy data through its `forward` method to verify the output shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7cda0b6e",
        "outputId": "a7773247-0b3e-4b99-a92f-787cbebf905d"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Define parameters for testing the Transformer\n",
        "SRC_VOCAB_SIZE = len(vocab) # English vocabulary size\n",
        "TRG_VOCAB_SIZE = len(vocab_es) # Spanish vocabulary size\n",
        "NUM_LAYERS = 2 # Number of encoder/decoder layers\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "model = Transformer(\n",
        "    src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    trg_vocab_size=TRG_VOCAB_SIZE,\n",
        "    embed_dim=EMBEDDING_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    forward_expansion=FORWARD_EXPANSION,\n",
        "    dropout=DROPOUT,\n",
        "    src_pad_idx=PAD_IDX,\n",
        "    trg_pad_idx=PAD_IDX,\n",
        "    max_len=MAX_SEQ_LEN\n",
        ")\n",
        "\n",
        "# Create dummy source and target inputs\n",
        "BATCH_SIZE_TEST = 2\n",
        "SRC_LEN_TEST = 10\n",
        "TRG_LEN_TEST = 12\n",
        "\n",
        "dummy_src_input = torch.randint(2, SRC_VOCAB_SIZE, (BATCH_SIZE_TEST, SRC_LEN_TEST))\n",
        "dummy_trg_input = torch.randint(2, TRG_VOCAB_SIZE, (BATCH_SIZE_TEST, TRG_LEN_TEST))\n",
        "\n",
        "# Replace some tokens with PAD_IDX to test masking\n",
        "dummy_src_input[0, 5:] = PAD_IDX\n",
        "dummy_trg_input[1, 7:] = PAD_IDX\n",
        "\n",
        "# Pass dummy inputs through the model\n",
        "output = model(dummy_src_input, dummy_trg_input)\n",
        "\n",
        "print(f\"Shape of dummy source input: {dummy_src_input.shape}\")\n",
        "print(f\"Shape of dummy target input: {dummy_trg_input.shape}\")\n",
        "print(f\"Shape of Transformer output: {output.shape}\")\n",
        "\n",
        "# Expected output shape: (BATCH_SIZE_TEST, TRG_LEN_TEST, TRG_VOCAB_SIZE)\n",
        "expected_output_shape = (BATCH_SIZE_TEST, TRG_LEN_TEST, TRG_VOCAB_SIZE)\n",
        "assert output.shape == expected_output_shape, f\"Output shape mismatch: Expected {expected_output_shape}, got {output.shape}\"\n",
        "\n",
        "print(\"Transformer model instantiated and tested successfully with expected output shape.\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'src_pad_idx' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3731259884.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Pass dummy inputs through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_src_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_trg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of dummy source input: {dummy_src_input.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3702144664.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Generate masks for source and target sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_src_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_trg_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_pad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'src_pad_idx' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82b0c8dc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `src_pad_idx` and `trg_pad_idx` were not accessed as instance attributes (`self.src_pad_idx`, `self.trg_pad_idx`) within the `forward` method of the `Transformer` class. I need to modify the `forward` method to correctly reference these parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47670e61",
        "outputId": "ec04c86e-c4d7-4038-e4b1-1e81c029953f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming MultiHeadSelfAttention, FeedForwardBlock, EncoderBlock, DecoderBlock, PositionalEncoding,\n",
        "# make_src_mask, make_trg_mask, vocab, vocab_es, EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT, PAD_IDX\n",
        "# are already defined or accessible in the global scope.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        trg_vocab_size,\n",
        "        embed_dim,\n",
        "        num_layers,\n",
        "        num_heads,\n",
        "        forward_expansion,\n",
        "        dropout,\n",
        "        src_pad_idx,\n",
        "        trg_pad_idx,\n",
        "        max_len=5000\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Store pad indices as instance attributes\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "        # Source and Target embedding layers\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_dim, padding_idx=self.src_pad_idx)\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embed_dim, padding_idx=self.trg_pad_idx)\n",
        "\n",
        "        # Positional encoding layer\n",
        "        self.positional_encoder = PositionalEncoding(embed_dim, max_len=max_len)\n",
        "\n",
        "        # Dropout layer for the output of positional encoding\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Encoder stack\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [\n",
        "                EncoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Decoder stack\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Final linear layer to project decoder output to target vocabulary size\n",
        "        self.fc_out = nn.Linear(embed_dim, trg_vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        # trg shape: (batch_size, trg_len)\n",
        "\n",
        "        # Generate masks for source and target sequences\n",
        "        src_mask = make_src_mask(src, self.src_pad_idx)\n",
        "        trg_mask = make_trg_mask(trg, self.trg_pad_idx)\n",
        "\n",
        "        # Apply source and target word embeddings\n",
        "        src_embeddings = self.src_word_embedding(src)\n",
        "        trg_embeddings = self.trg_word_embedding(trg)\n",
        "\n",
        "        # Apply positional encoding and dropout to embeddings\n",
        "        src_with_pos = self.dropout(self.positional_encoder(src_embeddings))\n",
        "        trg_with_pos = self.dropout(self.positional_encoder(trg_embeddings))\n",
        "\n",
        "        # Pass through encoder stack\n",
        "        encoder_output = src_with_pos\n",
        "        for layer in self.encoder_blocks:\n",
        "            encoder_output = layer(encoder_output, src_mask)\n",
        "\n",
        "        # Pass through decoder stack\n",
        "        decoder_output = trg_with_pos\n",
        "        for layer in self.decoder_blocks:\n",
        "            decoder_output = layer(decoder_output, encoder_output, trg_mask, src_mask)\n",
        "\n",
        "        # Apply final linear layer\n",
        "        final_output = self.fc_out(decoder_output)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "print(\"Transformer class defined (fixed).\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer class defined (fixed).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9adb61f"
      },
      "source": [
        "**Reasoning**:\n",
        "The `Transformer` class has been fixed. I will now re-test its functionality by creating dummy input data, masks, and then instantiating the class and passing the dummy data through its `forward` method to verify the output shape and confirm the fix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdc8dc50",
        "outputId": "247527c8-e7f4-49f1-8846-3398951f7b31"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Define parameters for testing the Transformer\n",
        "SRC_VOCAB_SIZE = len(vocab) # English vocabulary size\n",
        "TRG_VOCAB_SIZE = len(vocab_es) # Spanish vocabulary size\n",
        "NUM_LAYERS = 2 # Number of encoder/decoder layers\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "model = Transformer(\n",
        "    src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    trg_vocab_size=TRG_VOCAB_SIZE,\n",
        "    embed_dim=EMBEDDING_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    forward_expansion=FORWARD_EXPANSION,\n",
        "    dropout=DROPOUT,\n",
        "    src_pad_idx=PAD_IDX,\n",
        "    trg_pad_idx=PAD_IDX,\n",
        "    max_len=MAX_SEQ_LEN\n",
        ")\n",
        "\n",
        "# Create dummy source and target inputs\n",
        "BATCH_SIZE_TEST = 2\n",
        "SRC_LEN_TEST = 10\n",
        "TRG_LEN_TEST = 12\n",
        "\n",
        "dummy_src_input = torch.randint(2, SRC_VOCAB_SIZE, (BATCH_SIZE_TEST, SRC_LEN_TEST))\n",
        "dummy_trg_input = torch.randint(2, TRG_VOCAB_SIZE, (BATCH_SIZE_TEST, TRG_LEN_TEST))\n",
        "\n",
        "# Replace some tokens with PAD_IDX to test masking\n",
        "dummy_src_input[0, 5:] = PAD_IDX\n",
        "dummy_trg_input[1, 7:] = PAD_IDX\n",
        "\n",
        "# Pass dummy inputs through the model\n",
        "output = model(dummy_src_input, dummy_trg_input)\n",
        "\n",
        "print(f\"Shape of dummy source input: {dummy_src_input.shape}\")\n",
        "print(f\"Shape of dummy target input: {dummy_trg_input.shape}\")\n",
        "print(f\"Shape of Transformer output: {output.shape}\")\n",
        "\n",
        "# Expected output shape: (BATCH_SIZE_TEST, TRG_LEN_TEST, TRG_VOCAB_SIZE)\n",
        "expected_output_shape = (BATCH_SIZE_TEST, TRG_LEN_TEST, TRG_VOCAB_SIZE)\n",
        "assert output.shape == expected_output_shape, f\"Output shape mismatch: Expected {expected_output_shape}, got {output.shape}\"\n",
        "\n",
        "print(\"Transformer model instantiated and tested successfully with expected output shape.\")\n",
        "\n",
        "# --- MODIFIED: Print a sample of the output probabilities ---\n",
        "print(\"\\nSample output probabilities (first batch, first token, first 5 vocab entries):\\n\", output[0, 0, :5])\n",
        "print(\"\\nSample output probabilities (first batch, first token, last 5 vocab entries):\\n\", output[0, 0, -5:])\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy source input: torch.Size([2, 10])\n",
            "Shape of dummy target input: torch.Size([2, 12])\n",
            "Shape of Transformer output: torch.Size([2, 12, 66909])\n",
            "Transformer model instantiated and tested successfully with expected output shape.\n",
            "\n",
            "Sample output probabilities (first batch, first token, first 5 vocab entries):\n",
            " tensor([ 0.8560,  0.3654, -0.5145,  0.2641,  0.1001], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Sample output probabilities (first batch, first token, last 5 vocab entries):\n",
            " tensor([-0.3482, -0.1516,  0.8089,  0.8491, -0.1502], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4dd2a4b",
        "outputId": "32fd6e10-a01d-445f-b024-8e6b3b3095b3"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Get the integer-to-string mapping for the Spanish vocabulary\n",
        "itos_es = vocab_es.get_itos()\n",
        "\n",
        "# Get the indices of the top 5 predicted tokens for the first batch, first token\n",
        "# Since the output is logits, argmax directly gives the index of the highest logit\n",
        "# For sample output, we are taking the first token's output distribution\n",
        "top_5_indices_output_0_0 = torch.topk(output[0, 0, :], 5).indices\n",
        "\n",
        "# Get the bottom 5 predicted tokens for the first batch, first token\n",
        "bottom_5_indices_output_0_0 = torch.topk(output[0, 0, :], 5, largest=False).indices\n",
        "\n",
        "print(\"Top 5 predicted words for the first output token:\")\n",
        "for idx in top_5_indices_output_0_0:\n",
        "    word = itos_es[idx.item()]\n",
        "    probability_score = output[0, 0, idx.item()].item()\n",
        "    print(f\"  '{word}': {probability_score:.4f}\")\n",
        "\n",
        "print(\"\\nBottom 5 predicted words for the first output token:\")\n",
        "for idx in bottom_5_indices_output_0_0:\n",
        "    word = itos_es[idx.item()]\n",
        "    probability_score = output[0, 0, idx.item()].item()\n",
        "    print(f\"  '{word}': {probability_score:.4f}\")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 predicted words for the first output token:\n",
            "  'experimentarlas': 2.7971\n",
            "  'Lipietz': 2.3911\n",
            "  '칰nicamente': 2.3557\n",
            "  'salidas': 2.3316\n",
            "  'pequen칤simo': 2.3028\n",
            "\n",
            "Bottom 5 predicted words for the first output token:\n",
            "  '86/611': -2.4535\n",
            "  'Riksdag': -2.4518\n",
            "  'supresi칩n': -2.3824\n",
            "  'concluded': -2.2721\n",
            "  'hallo': -2.2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eebbfa"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the successful implementation and testing of the full Transformer model class, noting its readiness for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "998c5da8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Source Mask Function (`make_src_mask`)**: The `make_src_mask` function was successfully defined and verified. It correctly generates a padding mask of shape `(batch_size, 1, 1, src_len)` to prevent attention to padding tokens in the encoder's source input.\n",
        "*   **Target Mask Function (`make_trg_mask`)**:\n",
        "    *   An initial implementation of `make_trg_mask` had a bug where the padding mask logic was incorrect for query positions that were themselves padding tokens, leading to an `AssertionError`.\n",
        "    *   The function was corrected to ensure proper masking when either the query or key token is padding, resulting in a combined padding and causal (look-ahead) mask of shape `(batch_size, 1, trg_len, trg_len)`. The corrected function passed all verification tests.\n",
        "*   **Transformer Model Implementation**:\n",
        "    *   The initial `Transformer` class implementation contained a `NameError` because the `src_pad_idx` and `trg_pad_idx` were not accessible within the `forward` method as instance attributes.\n",
        "    *   This issue was resolved by storing `src_pad_idx` and `trg_pad_idx` as `self` attributes in the `__init__` method.\n",
        "    *   The full `Transformer` model, integrating `EncoderBlock`, `DecoderBlock`, embedding layers, positional encodings, and the mask functions, was successfully instantiated and tested with dummy inputs. The model produced the expected output shape of `(BATCH_SIZE, TRG_LEN, TRG_VOCAB_SIZE)`, confirming its correct functionality and integration of all components.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The fully implemented Transformer model is robust, with all its core components (embedding layers, positional encoding, encoder/decoder blocks, and masking mechanisms) successfully integrated and verified, making it ready for the training phase.\n",
        "*   The next logical step is to prepare a training loop, define a loss function and optimizer, and then train the Transformer model on a suitable dataset for a specific sequence-to-sequence task, such as machine translation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109cd13c"
      },
      "source": [
        "# Task\n",
        "Split the numericalized English (source) and Spanish (target) sentences into training and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bcc452"
      },
      "source": [
        "## Split Data into Train and Test Sets\n",
        "\n",
        "### Subtask:\n",
        "Split the numericalized English (source) and Spanish (target) sentences into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cf71578"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the `train_test_split` function to split the data and then apply it to the English and Spanish numericalized sequences, ensuring corresponding pairs are maintained, and then print the shapes of the resulting datasets for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35a25f46",
        "outputId": "015e11e1-b3f7-4779-9398-5792fc84827c"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Combine English and Spanish sequences to ensure they are split together\n",
        "# This assumes input_sequences_numerical and output_sequences_numerical have the same number of samples\n",
        "combined_data = list(zip(input_sequences_numerical, output_sequences_numerical))\n",
        "\n",
        "# Split the combined data into training and testing sets\n",
        "# train_test_split will split the list of (src, trg) tuples\n",
        "src_trg_train, src_trg_test = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Unzip the split data back into separate source and target tensors\n",
        "src_train, trg_train = zip(*src_trg_train)\n",
        "src_test, trg_test = zip(*src_trg_test)\n",
        "\n",
        "# Convert lists of tensors back to single tensors\n",
        "src_train = torch.stack(src_train)\n",
        "trg_train = torch.stack(trg_train)\n",
        "src_test = torch.stack(src_test)\n",
        "trg_test = torch.stack(trg_test)\n",
        "\n",
        "print(f\"Shape of src_train: {src_train.shape}\")\n",
        "print(f\"Shape of trg_train: {trg_train.shape}\")\n",
        "print(f\"Shape of src_test: {src_test.shape}\")\n",
        "print(f\"Shape of trg_test: {trg_test.shape}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of src_train: torch.Size([119647, 100])\n",
            "Shape of trg_train: torch.Size([119647, 100])\n",
            "Shape of src_test: torch.Size([29912, 100])\n",
            "Shape of trg_test: torch.Size([29912, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404da32c"
      },
      "source": [
        "## Prepare DataLoaders for Training and Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Create `torch.utils.data.DataLoader` instances for both the training and testing datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6d224f2"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to prepare `DataLoader` instances for both the training and testing datasets using `TensorDataset` and `DataLoader` from `torch.utils.data`, and then print their lengths to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ba3cbbd",
        "outputId": "c3bca198-a73f-4b05-8a3f-83d41e4f7242"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 2. Create a TensorDataset object for the training data\n",
        "train_dataset = TensorDataset(src_train, trg_train)\n",
        "\n",
        "# 3. Create a TensorDataset object for the testing data\n",
        "test_dataset = TensorDataset(src_test, trg_test)\n",
        "\n",
        "# 4. Create a DataLoader for the training dataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# 5. Create a DataLoader for the testing dataset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 6. Print the lengths of the training and testing DataLoaders\n",
        "print(f\"Number of batches in training DataLoader: {len(train_dataloader)}\")\n",
        "print(f\"Number of batches in testing DataLoader: {len(test_dataloader)}\")\n",
        "\n",
        "# Print shapes of a sample batch to verify\n",
        "for src_batch, trg_batch in train_dataloader:\n",
        "    print(f\"\\nShape of source batch from training DataLoader: {src_batch.shape}\")\n",
        "    print(f\"Shape of target batch from training DataLoader: {trg_batch.shape}\")\n",
        "    break\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in training DataLoader: 3739\n",
            "Number of batches in testing DataLoader: 935\n",
            "\n",
            "Shape of source batch from training DataLoader: torch.Size([32, 100])\n",
            "Shape of target batch from training DataLoader: torch.Size([32, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8171a204"
      },
      "source": [
        "## Define Loss Function and Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Define the appropriate loss function for sequence-to-sequence tasks (e.g., `nn.CrossEntropyLoss`) and select an optimizer (e.g., `torch.optim.Adam`) to update the model's weights during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "547afa94"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the loss function and optimizer as specified in the instructions. This involves importing `torch.nn` and `torch.optim`, instantiating `nn.CrossEntropyLoss` with `ignore_index=PAD_IDX`, and initializing `torch.optim.Adam` with the model's parameters and a learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "949f41a7",
        "outputId": "1ad95b1a-8f58-49d1-a885-694a088ad4ff"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function\n",
        "# CrossEntropyLoss is suitable for classification tasks (predicting the next token).\n",
        "# ignore_index=PAD_IDX tells the loss function to ignore calculations for padding tokens.\n",
        "# This is important because padding tokens are not actual words and should not contribute to the loss.\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "# Define the optimizer\n",
        "# Adam optimizer is a popular choice for deep learning models.\n",
        "# model.parameters() provides all trainable parameters of our Transformer model.\n",
        "# A learning rate (lr) of 1e-4 is a common starting point.\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Loss function (CrossEntropyLoss) and optimizer (Adam) defined.\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function (CrossEntropyLoss) and optimizer (Adam) defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5635c65c"
      },
      "source": [
        "## Implement Training Step\n",
        "\n",
        "### Subtask:\n",
        "Create a function that encapsulates a single training step: performing a forward pass, calculating the loss, executing a backward pass to compute gradients, and updating the model's parameters using the optimizer. This function will also handle masking for both source and target sequences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e5abaa"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `train_step` function that encapsulates a single training step, including the forward pass, loss calculation, backward pass, and parameter update, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "016a9659",
        "outputId": "c770cdf1-2400-4a45-f05a-f4b41bec587d"
      },
      "source": [
        "def train_step(model, src, trg, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Transformer model.\n",
        "        src (torch.Tensor): The source sequence batch (batch_size, src_len).\n",
        "        trg (torch.Tensor): The target sequence batch (batch_size, trg_len).\n",
        "        optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n",
        "        criterion (nn.Module): The loss function.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated loss for the training step.\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Zero out the gradients\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(src, trg[:, :-1]) # Exclude <eos> from target input\n",
        "\n",
        "    # Reshape output and target for loss calculation\n",
        "    # The output is (batch_size, trg_len-1, trg_vocab_size)\n",
        "    # The target is (batch_size, trg_len)\n",
        "    # We want to predict the next token, so shift trg by one position for loss calculation\n",
        "    # output: (batch_size * (trg_len-1), trg_vocab_size)\n",
        "    # trg: (batch_size * (trg_len-1))\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output.reshape(-1, output_dim)\n",
        "    trg = trg[:, 1:].reshape(-1) # Exclude <bos> from target for loss\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = criterion(output, trg)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "print(\"train_step function defined.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_step function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "534ea1fb"
      },
      "source": [
        "## Implement Evaluation Step\n",
        "\n",
        "### Subtask:\n",
        "Create a function for a single evaluation step. This will perform a forward pass and calculate the loss on the validation set without updating model parameters (i.e., in `eval()` mode). This function will also handle masking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b11f6e"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `evaluate_step` function as instructed, which will perform a forward pass and calculate the loss on the validation set without updating model parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eabbca12",
        "outputId": "17e4ac43-c95e-4c44-9736-89d67668f151"
      },
      "source": [
        "def evaluate_step(model, src, trg, criterion):\n",
        "    \"\"\"\n",
        "    Performs a single evaluation step for the Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The Transformer model.\n",
        "        src (torch.Tensor): The source sequence batch (batch_size, src_len).\n",
        "        trg (torch.Tensor): The target sequence batch (batch_size, trg_len).\n",
        "        criterion (nn.Module): The loss function.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The calculated loss for the evaluation step.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        # Forward pass\n",
        "        output = model(src, trg[:, :-1]) # Exclude <eos> from target input\n",
        "\n",
        "        # Reshape output and target for loss calculation\n",
        "        # The output is (batch_size, trg_len-1, trg_vocab_size)\n",
        "        # The target is (batch_size, trg_len)\n",
        "        # We want to predict the next token, so shift trg by one position for loss calculation\n",
        "        # output: (batch_size * (trg_len-1), trg_vocab_size)\n",
        "        # trg: (batch_size * (trg_len-1))\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1) # Exclude <bos> from target for loss\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "    return loss\n",
        "\n",
        "print(\"evaluate_step function defined.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluate_step function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17da527f"
      },
      "source": [
        "## Implement Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Construct the main training loop. This loop will iterate through a specified number of epochs, calling the training step for each batch in the training data and the evaluation step for the test/validation data. It will also track and potentially print training and validation losses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1694c58d"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to construct the main training loop according to the provided instructions, which involves setting up the device, iterating through epochs, performing training and evaluation steps, and tracking losses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "8a623ee6",
        "outputId": "d57adeac-2496-4e2e-f101-2bca4bdf0240"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# 1. Determine if a GPU is available and set the device accordingly\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Move the model to the chosen device\n",
        "model.to(device)\n",
        "\n",
        "# 3. Define the number of training epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# 4. Initialize empty lists to store training and validation losses for each epoch\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
        "\n",
        "# Start a loop that iterates NUM_EPOCHS times\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    start_time = time.time()\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_eval_loss = 0.0\n",
        "\n",
        "    # 7. For the training phase of each epoch:\n",
        "    model.train() # Ensure model is in training mode\n",
        "    for batch_idx, (src, trg) in enumerate(train_dataloader):\n",
        "        # b. Move src and trg to the device\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        # c. Call the train_step function\n",
        "        loss = train_step(model, src, trg, optimizer, criterion)\n",
        "\n",
        "        # d. Add the returned loss to epoch_train_loss\n",
        "        epoch_train_loss += loss.item()\n",
        "\n",
        "    # 8. For the evaluation phase of each epoch:\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, trg) in enumerate(test_dataloader):\n",
        "            # b. Move src and trg to the device\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            # c. Call the evaluate_step function\n",
        "            loss = evaluate_step(model, src, trg, criterion)\n",
        "\n",
        "            # d. Add the returned loss to epoch_eval_loss\n",
        "            epoch_eval_loss += loss.item()\n",
        "\n",
        "    # 9. Calculate the average training and evaluation losses for the epoch.\n",
        "    avg_train_loss = epoch_train_loss / len(train_dataloader)\n",
        "    avg_eval_loss = epoch_eval_loss / len(test_dataloader)\n",
        "\n",
        "    # 10. Print the epoch number, average training loss, and average evaluation loss.\n",
        "    end_time = time.time()\n",
        "    epoch_mins = int((end_time - start_time) / 60)\n",
        "    epoch_secs = int((end_time - start_time) % 60)\n",
        "\n",
        "    print(f'Epoch: {epoch:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {avg_train_loss:.3f}')\n",
        "    print(f'\\t Eval. Loss: {avg_eval_loss:.3f}')\n",
        "\n",
        "    # 11. Store the average training and evaluation losses in their respective lists.\n",
        "    train_losses.append(avg_train_loss)\n",
        "    eval_losses.append(avg_eval_loss)\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting training for 10 epochs...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3520970196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# c. Call the train_step function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# d. Add the returned loss to epoch_train_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-822103514.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, src, trg, optimizer, criterion)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Backward pass and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}