{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charchar1245/Transformer-for-Translating-English-to-Spanish/blob/main/EnglishtoSpanishTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding data for the model -- English Spanish Translation"
      ],
      "metadata": {
        "id": "C-9LBaVPNqT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchtext torchaudio\n",
        "!pip install torch==2.2.2 torchtext==0.17.2 torchaudio==2.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "Rvpp23OSOGao",
        "outputId": "5107d73b-ec23-4807-eabf-d6d638fab710"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.2\n",
            "Uninstalling torch-2.2.2:\n",
            "  Successfully uninstalled torch-2.2.2\n",
            "Found existing installation: torchtext 0.17.2\n",
            "Uninstalling torchtext-0.17.2:\n",
            "  Successfully uninstalled torchtext-0.17.2\n",
            "Found existing installation: torchaudio 2.2.2\n",
            "Uninstalling torchaudio-2.2.2:\n",
            "  Successfully uninstalled torchaudio-2.2.2\n",
            "Collecting torch==2.2.2\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchtext==0.17.2\n",
            "  Using cached torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torchaudio==2.2.2\n",
            "  Using cached torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.17.2) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.9.86)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.17.2) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "Using cached torchtext-0.17.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Using cached torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl (3.3 MB)\n",
            "Installing collected packages: torch, torchtext, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.24.0+cpu requires torch==2.9.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.2 torchaudio-2.2.2 torchtext-0.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchtext"
                ]
              },
              "id": "9d59e5107e48468b8e91ab1c3c6b9c6c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "https://huggingface.co/datasets/okezieowen/english_to_spanish"
      ],
      "metadata": {
        "id": "57nqSDA5QV9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "ds = load_dataset(\"okezieowen/english_to_spanish\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW2Ujag6QBar",
        "outputId": "58fdd9f1-0547-429d-e34d-94247d63b20f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-504350806.py\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# To see the head of the dataset, you need to access a specific split first.\n",
        "# For example, to see the first 10 elements of the 'train' split:\n",
        "if 'train' in ds:\n",
        "    selected_data = ds['train'].select(range(10))\n",
        "    df = pd.DataFrame(selected_data)\n",
        "    print(f\"Displaying first 10 examples from the 'train' split:\")\n",
        "    display(df)\n",
        "elif len(ds) > 0:\n",
        "    # If 'train' split is not available, try to get the first available split\n",
        "    first_split_name = list(ds.keys())[0]\n",
        "    selected_data = ds[first_split_name].select(range(10))\n",
        "    df = pd.DataFrame(selected_data)\n",
        "    print(f\"Showing first 10 examples from the '{first_split_name}' split:\")\n",
        "    display(df)\n",
        "else:\n",
        "    print(\"DatasetDict is empty or has no accessible splits.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "uBx8331pT97A",
        "outputId": "216a1213-d926-414a-ffff-84bd9abe6a5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying first 10 examples from the 'train' split:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             English  \\\n",
              "0  I declare resumed the session of the European ...   \n",
              "1  Although, as you will have seen, the dreaded '...   \n",
              "2  You have requested a debate on this subject in...   \n",
              "3  In the meantime, I should like to observe a mi...   \n",
              "4     Please rise, then, for this minute' s silence.   \n",
              "5  (The House rose and observed a minute' s silence)   \n",
              "6              Madam President, on a point of order.   \n",
              "7  You will be aware from the press and televisio...   \n",
              "8  One of the people assassinated very recently i...   \n",
              "9  Would it be appropriate for you, Madam Preside...   \n",
              "\n",
              "                                             Spanish  \n",
              "0  Declaro reanudado el per칤odo de sesiones del P...  \n",
              "1  Como todos han podido comprobar, el gran \"efec...  \n",
              "2  Sus Se침or칤as han solicitado un debate sobre el...  \n",
              "3  A la espera de que se produzca, de acuerdo con...  \n",
              "4  Invito a todos a que nos pongamos de pie para ...  \n",
              "5  (El Parlamento, de pie, guarda un minuto de si...  \n",
              "6  Se침ora Presidenta, una cuesti칩n de procedimiento.  \n",
              "7  Sabr치 usted por la prensa y la televisi칩n que ...  \n",
              "8  Una de las personas que recientemente han ases...  \n",
              "9  쯉er칤a apropiado que usted, Se침ora Presidenta,...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28845b9b-b262-4a2a-83a5-5501854e7248\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Spanish</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I declare resumed the session of the European ...</td>\n",
              "      <td>Declaro reanudado el per칤odo de sesiones del P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Although, as you will have seen, the dreaded '...</td>\n",
              "      <td>Como todos han podido comprobar, el gran \"efec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You have requested a debate on this subject in...</td>\n",
              "      <td>Sus Se침or칤as han solicitado un debate sobre el...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>In the meantime, I should like to observe a mi...</td>\n",
              "      <td>A la espera de que se produzca, de acuerdo con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please rise, then, for this minute' s silence.</td>\n",
              "      <td>Invito a todos a que nos pongamos de pie para ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(The House rose and observed a minute' s silence)</td>\n",
              "      <td>(El Parlamento, de pie, guarda un minuto de si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Madam President, on a point of order.</td>\n",
              "      <td>Se침ora Presidenta, una cuesti칩n de procedimiento.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>You will be aware from the press and televisio...</td>\n",
              "      <td>Sabr치 usted por la prensa y la televisi칩n que ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>One of the people assassinated very recently i...</td>\n",
              "      <td>Una de las personas que recientemente han ases...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Would it be appropriate for you, Madam Preside...</td>\n",
              "      <td>쯉er칤a apropiado que usted, Se침ora Presidenta,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28845b9b-b262-4a2a-83a5-5501854e7248')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28845b9b-b262-4a2a-83a5-5501854e7248 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28845b9b-b262-4a2a-83a5-5501854e7248');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f172246e-9b31-434d-ac6c-a8a80f29ed1f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f172246e-9b31-434d-ac6c-a8a80f29ed1f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f172246e-9b31-434d-ac6c-a8a80f29ed1f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f6ae6610-de2a-4a6e-98cd-2bc83679238c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f6ae6610-de2a-4a6e-98cd-2bc83679238c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"English\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\",\n          \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n          \"(The House rose and observed a minute' s silence)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spanish\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visit\\u00f3 el Parlamento Europeo.\",\n          \"Como todos han podido comprobar, el gran \\\"efecto del a\\u00f1o 2000\\\" no se ha producido. En cambio, los ciudadanos de varios de nuestros pa\\u00edses han sido v\\u00edctimas de cat\\u00e1strofes naturales verdaderamente terribles.\",\n          \"(El Parlamento, de pie, guarda un minuto de silencio)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cd65b44",
        "outputId": "20bc3cc7-a718-4d17-9d73-b9b630ddd24c"
      },
      "source": [
        "# Ensure spaCy English model is loaded if not already\n",
        "# If you encounter an error, uncomment and run the following line:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "# spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the tokenization function (if not already defined)\n",
        "def tokenize_en(text):\n",
        "    \"\"\"Tokenizes English text using a spaCy en_core_web_sm model and returns a list of tokens.\"\"\"\n",
        "    # Check if spacy_en is defined globally, otherwise load it\n",
        "    if 'spacy_en' not in globals():\n",
        "        global spacy_en\n",
        "        import spacy\n",
        "        spacy_en = spacy.load('en_core_web_sm')\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Apply tokenization to the 'English' column of the 'train' split\n",
        "# Filter out None values before tokenizing\n",
        "tokenized_english_sentences = [tokenize_en(sentence) for sentence in ds['train']['English'] if sentence is not None]\n",
        "\n",
        "# Display the first 5 tokenized English sentences\n",
        "print(\"First 5 tokenized English sentences:\")\n",
        "for i, tokens in enumerate(tokenized_english_sentences[:5]):\n",
        "    print(tokens)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 tokenized English sentences:\n",
            "['I', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'European', 'Parliament', 'adjourned', 'on', 'Friday', '17', 'December', '1999', ',', 'and', 'I', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period', '.']\n",
            "['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'\", 'millennium', 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n",
            "['You', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', ',', 'during', 'this', 'part', '-', 'session', '.']\n",
            "['In', 'the', 'meantime', ',', 'I', 'should', 'like', 'to', 'observe', 'a', 'minute', \"'\", 's', 'silence', ',', 'as', 'a', 'number', 'of', 'Members', 'have', 'requested', ',', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', ',', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', ',', 'in', 'the', 'various', 'countries', 'of', 'the', 'European', 'Union', '.']\n",
            "['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd5f5bcb"
      },
      "source": [
        "First, let's build a vocabulary from our `tokenized_english_sentences`. This vocabulary will map each unique word to a unique integer ID."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d86cc4",
        "outputId": "540419e9-8112-4944-e8ad-8e86bb95d56a"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Flatten the list of tokenized sentences to get all tokens\n",
        "all_tokens = [token for sentence in tokenized_english_sentences for token in sentence]\n",
        "\n",
        "# Build vocabulary with special tokens\n",
        "UNK_IDX = 0 # Unknown token index\n",
        "PAD_IDX = 1 # Padding token index\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    [sentence for sentence in tokenized_english_sentences], # Pass an iterator of token lists\n",
        "    min_freq=1,\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example mapping: 'hello' -> {vocab(['hello'])[0]}, '<pad>' -> {vocab(['<pad>'])[0]}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 37310\n",
            "Example mapping: 'hello' -> 33348, '<pad>' -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5a7f6b6"
      },
      "source": [
        "Next, we'll numericalize the tokenized sentences (convert tokens to their vocabulary IDs) and then pad them to a consistent length. We'll choose a maximum sequence length, and either pad shorter sentences or truncate longer ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1c6764",
        "outputId": "4dd15447-8eb8-4ecb-f4cc-0e45551b1c50"
      },
      "source": [
        "MAX_SEQ_LEN = 100 # Define a maximum sequence length\n",
        "\n",
        "def numericalize_and_pad(sentences, vocab, max_seq_len, pad_idx=PAD_IDX):\n",
        "    numericalized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Add <bos> and <eos> tokens\n",
        "        indexed_sentence = [vocab['<bos>']] + vocab(sentence) + [vocab['<eos>']]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(indexed_sentence) < max_seq_len:\n",
        "            padded_sentence = indexed_sentence + [pad_idx] * (max_seq_len - len(indexed_sentence))\n",
        "        else:\n",
        "            padded_sentence = indexed_sentence[:max_seq_len]\n",
        "\n",
        "        numericalized_sentences.append(padded_sentence)\n",
        "    return torch.tensor(numericalized_sentences)\n",
        "\n",
        "# Numericalize and pad the English sentences\n",
        "input_sequences_numerical = numericalize_and_pad(tokenized_english_sentences, vocab, MAX_SEQ_LEN, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of numerical input sequences: {input_sequences_numerical.shape}\")\n",
        "print(\"First numerical input sequence:\", input_sequences_numerical[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of numerical input sequences: torch.Size([149829, 100])\n",
            "First numerical input sequence: tensor([    2,    14,  2756,  2673,     4,  1028,     7,     4,    30,    50,\n",
            "         4451,    16,  2462,  2104,  1083,   587,     5,     9,    14,    36,\n",
            "           62,   367,   261,     8,   307,    52,    12,  1472,    96,   166,\n",
            "           10,     4,   198,    13,    52,  4087,    12,  6453, 18740,   537,\n",
            "            6,     3,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42d7b2b"
      },
      "source": [
        "Now, we define an embedding layer to convert these numerical IDs into dense vector representations. After that, we'll add positional encodings, which are crucial for transformers to understand the order of words in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deb4e618",
        "outputId": "eee3b928-6a48-4a38-e721-56fef3e2dd01"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "EMBEDDING_DIM = 256 # Dimension of word embeddings\n",
        "\n",
        "# Create the embedding layer\n",
        "embedding_layer = nn.Embedding(len(vocab), EMBEDDING_DIM, padding_idx=PAD_IDX)\n",
        "\n",
        "BATCH_SIZE = 32 # Batch size\n",
        "dataset = TensorDataset(input_sequences_numerical)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Pass the numerical sequences through the embedding layer\n",
        "# input_embeddings = embedding_layer(input_sequences_numerical)\n",
        "for batch_idx, (batch,) in enumerate(dataloader):\n",
        "    # batch: (batch_size, seq_len)\n",
        "\n",
        "    input_embeddings = embedding_layer(batch)\n",
        "    # input_embeddings: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "    # 游댳 later: positional encoding, attention, loss, backprop\n",
        "\n",
        "    if batch_idx == 0:\n",
        "        print(input_embeddings.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Shape of input embeddings: {input_embeddings.shape}\") # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "# Define Positional Encoding module\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0., max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        # self.pe: (1, max_len, d_model)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "# Instantiate and apply positional encoding\n",
        "positional_encoder = PositionalEncoding(EMBEDDING_DIM, MAX_SEQ_LEN)\n",
        "input_embeddings_with_pos = positional_encoder(input_embeddings)\n",
        "\n",
        "print(f\"Shape of input embeddings with positional encoding: {input_embeddings_with_pos.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Jyx6lSnD2D",
        "outputId": "eeb350c3-0b4d-4326-a7c3-a53354e294cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input embeddings: torch.Size([5, 100, 256])\n",
            "Shape of input embeddings with positional encoding: torch.Size([5, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ada4bf8"
      },
      "source": [
        "First, let's define the `MultiHeadSelfAttention` module. This module allows the model to jointly attend to information from different representation subspaces at different positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af084bc6"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_dim\n",
        "        ), \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_dim)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = value.shape[1], key.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.num_heads different pieces\n",
        "        value = value.reshape(N, value_len, self.num_heads, self.head_dim)\n",
        "        key = key.reshape(N, key_len, self.num_heads, self.head_dim)\n",
        "        query = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        values = self.values(value)\n",
        "        keys = self.keys(key)\n",
        "        queries = self.queries(query)\n",
        "\n",
        "        # Einsum does matrix multiplication for query * key.T\n",
        "        # with shape (N, heads, query_len, head_dim) * (N, heads, head_dim, key_len) -> (N, heads, query_len, key_len)\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# num_heads = 8 # Number of attention heads\n",
        "\n",
        "# value = torch.randn(N, seq_len, embed_dim)\n",
        "# key = torch.randn(N, seq_len, embed_dim)\n",
        "# query = torch.randn(N, seq_len, embed_dim)\n",
        "# mask = torch.ones(N, 1, seq_len, seq_len) # Example mask (no masking for simplicity)\n",
        "\n",
        "# attention_block = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "# output = attention_block(value, key, query, mask)\n",
        "# print(f\"Output shape of MultiHeadSelfAttention: {output.shape}\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1eb8ab"
      },
      "source": [
        "Next, we define a simple `FeedForwardBlock`, which is applied to each position separately and identically. This typically consists of two linear transformations with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c93af077"
      },
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, forward_expansion):\n",
        "        super(FeedForwardBlock, self).__init__()\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * forward_expansion),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * forward_expansion, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.feed_forward(x)\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# forward_expansion = 4\n",
        "\n",
        "# x = torch.randn(N, seq_len, embed_dim)\n",
        "\n",
        "# ff_block = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "# output = ff_block(x)\n",
        "# print(f\"Output shape of FeedForwardBlock: {output.shape}\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5b2594"
      },
      "source": [
        "Finally, we combine these two components into an `EncoderBlock`. This block integrates multi-head self-attention, a feed-forward network, residual connections, and layer normalization, forming a fundamental building block of a Transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "196da9c0"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Multi-head self-attention with residual connection and layer normalization\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention + x))\n",
        "\n",
        "        # Feed-forward network with residual connection and layer normalization\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(feed_forward + x))\n",
        "        return out\n",
        "\n",
        "# Example Usage:\n",
        "# N = 2 # Batch size\n",
        "# seq_len = 10 # Sequence length\n",
        "# embed_dim = 256 # Embedding dimension\n",
        "# num_heads = 8 # Number of attention heads\n",
        "# forward_expansion = 4 # Expansion factor for feed-forward layer\n",
        "# dropout = 0.1 # Dropout rate\n",
        "\n",
        "# x = torch.randn(N, seq_len, embed_dim)\n",
        "# mask = torch.ones(N, 1, seq_len, seq_len) # Example mask\n",
        "\n",
        "# encoder_block = EncoderBlock(embed_dim, num_heads, forward_expansion, dropout)\n",
        "# output = encoder_block(x, mask)\n",
        "# print(f\"Output shape of EncoderBlock: {output.shape}\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebcdeac9"
      },
      "source": [
        "Let's test the `EncoderBlock` with the `input_embeddings_with_pos` we generated earlier. Note that `input_embeddings_with_pos` had a batch size of 5 from the previous `dataloader` iteration, not the full `BATCH_SIZE` of 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f7d023",
        "outputId": "c2827e4f-3d13-402d-9f40-fce2eef33d08"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Redefine the batch size to match what was in input_embeddings_with_pos\n",
        "# The previous run only processed 5 items in the last batch if the dataset size was not divisible by BATCH_SIZE\n",
        "# Or, to be safe, let's just make a dummy input with the current batch size used in input_embeddings_with_pos\n",
        "current_batch_size = input_embeddings_with_pos.shape[0]\n",
        "\n",
        "# Assuming input_embeddings_with_pos comes from the previous run with a certain batch size\n",
        "# We will use this size for our dummy input to test the EncoderBlock\n",
        "\n",
        "# Set parameters for the EncoderBlock\n",
        "EMBEDDING_DIM = input_embeddings_with_pos.shape[2] # Use the actual embedding dimension\n",
        "NUM_HEADS = 8  # A common choice, adjust as needed\n",
        "FORWARD_EXPANSION = 4 # A common choice, adjust as needed\n",
        "DROPOUT = 0.1 # A common choice, adjust as needed\n",
        "\n",
        "# Create a dummy mask for testing. For an encoder, this is typically all ones (no masking within sequence).\n",
        "# Or it can be used for padding mask, where padded tokens are masked out.\n",
        "# Here we assume no padding mask for simplicity in this test.\n",
        "mask = torch.ones(current_batch_size, 1, MAX_SEQ_LEN, MAX_SEQ_LEN)\n",
        "\n",
        "# Instantiate the EncoderBlock\n",
        "encoder_block = EncoderBlock(EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT)\n",
        "\n",
        "# Pass the input embeddings with positional encoding through the EncoderBlock\n",
        "output_encoder_block = encoder_block(input_embeddings_with_pos, mask)\n",
        "\n",
        "print(f\"Shape of output after EncoderBlock: {output_encoder_block.shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output after EncoderBlock: torch.Size([5, 100, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8598b10"
      },
      "source": [
        "# Task\n",
        "Tokenize the Spanish sentences in the dataset using spaCy's `es_core_news_sm` model, then build a Spanish vocabulary, and numericalize and pad the tokenized Spanish sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6565cb2"
      },
      "source": [
        "## Tokenize Spanish Sentences\n",
        "\n",
        "### Subtask:\n",
        "Define a tokenization function for Spanish text using spaCy's es_core_news_sm model and apply it to the Spanish column of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b36f17"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define a tokenization function for Spanish text using spaCy and apply it to the 'Spanish' column of the dataset. This involves importing spaCy, defining the `tokenize_es` function to handle model loading and tokenization, and then applying this function to the relevant dataset column while handling potential `None` values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "e4c63feb",
        "outputId": "5f37df6c-ea8f-4bb6-b074-58bec4ff1397"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Ensure spaCy Spanish model is loaded if not already\n",
        "# If you encounter an error, uncomment and run the following line:\n",
        "# !python -m spacy download es_core_news_sm\n",
        "\n",
        "# Define the tokenization function for Spanish\n",
        "def tokenize_es(text):\n",
        "    \"\"\"Tokenizes Spanish text using a spaCy es_core_news_sm model and returns a list of tokens.\"\"\"\n",
        "    # Check if spacy_es is defined globally, otherwise load it\n",
        "    if 'spacy_es' not in globals():\n",
        "        global spacy_es\n",
        "        spacy_es = spacy.load('es_core_news_sm')\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "# Apply tokenization to the 'Spanish' column of the 'train' split\n",
        "# Filter out None values before tokenizing\n",
        "tokenized_spanish_sentences = [tokenize_es(sentence) for sentence in ds['train']['Spanish'] if sentence is not None]\n",
        "\n",
        "# Display the first 5 tokenized Spanish sentences\n",
        "print(\"First 5 tokenized Spanish sentences:\")\n",
        "for i, tokens in enumerate(tokenized_spanish_sentences[:5]):\n",
        "    print(tokens)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2032993973.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Apply tokenization to the 'Spanish' column of the 'train' split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Filter out None values before tokenizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtokenized_spanish_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize_es\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Spanish'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Display the first 5 tokenized Spanish sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2032993973.py\u001b[0m in \u001b[0;36mtokenize_es\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'spacy_es'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mspacy_es\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mspacy_es\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_es\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48bcc30b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous cell failed because the `es_core_news_sm` spaCy model was not found. I need to install it before attempting to load it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2ddf8d",
        "outputId": "e452cdc6-7872-4cd9-db91-f9db53ac7676"
      },
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Define the tokenization function for Spanish\n",
        "def tokenize_es(text):\n",
        "    \"\"\"Tokenizes Spanish text using a spaCy es_core_news_sm model and returns a list of tokens.\"\"\"\n",
        "    # Check if spacy_es is defined globally, otherwise load it\n",
        "    if 'spacy_es' not in globals():\n",
        "        global spacy_es\n",
        "        spacy_es = spacy.load('es_core_news_sm')\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "# Apply tokenization to the 'Spanish' column of the 'train' split\n",
        "# Filter out None values before tokenizing\n",
        "tokenized_spanish_sentences = [tokenize_es(sentence) for sentence in ds['train']['Spanish'] if sentence is not None]\n",
        "\n",
        "# Display the first 5 tokenized Spanish sentences\n",
        "print(\"First 5 tokenized Spanish sentences:\")\n",
        "for i, tokens in enumerate(tokenized_spanish_sentences[:5]):\n",
        "    print(tokens)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
            "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/spacy/compat.py\", line 5, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/types.py\", line 27, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/thinc/compat.py\", line 35, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1477, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較較\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m九 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m丘 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "First 5 tokenized Spanish sentences:\n",
            "['Declaro', 'reanudado', 'el', 'per칤odo', 'de', 'sesiones', 'del', 'Parlamento', 'Europeo', ',', 'interrumpido', 'el', 'viernes', '17', 'de', 'diciembre', 'pasado', ',', 'y', 'reitero', 'a', 'Sus', 'Se침or칤as', 'mi', 'deseo', 'de', 'que', 'hayan', 'tenido', 'unas', 'buenas', 'vacaciones', '.']\n",
            "['Como', 'todos', 'han', 'podido', 'comprobar', ',', 'el', 'gran', '\"', 'efecto', 'del', 'a침o', '2000', '\"', 'no', 'se', 'ha', 'producido', '.', 'En', 'cambio', ',', 'los', 'ciudadanos', 'de', 'varios', 'de', 'nuestros', 'pa칤ses', 'han', 'sido', 'v칤ctimas', 'de', 'cat치strofes', 'naturales', 'verdaderamente', 'terribles', '.']\n",
            "['Sus', 'Se침or칤as', 'han', 'solicitado', 'un', 'debate', 'sobre', 'el', 'tema', 'para', 'los', 'pr칩ximos', 'd칤as', ',', 'en', 'el', 'curso', 'de', 'este', 'per칤odo', 'de', 'sesiones', '.']\n",
            "['A', 'la', 'espera', 'de', 'que', 'se', 'produzca', ',', 'de', 'acuerdo', 'con', 'muchos', 'colegas', 'que', 'me', 'lo', 'han', 'pedido', ',', 'pido', 'que', 'hagamos', 'un', 'minuto', 'de', 'silencio', 'en', 'memoria', 'de', 'todas', 'las', 'v칤ctimas', 'de', 'las', 'tormentas', ',', 'en', 'los', 'distintos', 'pa칤ses', 'de', 'la', 'Uni칩n', 'Europea', 'afectados', '.']\n",
            "['Invito', 'a', 'todos', 'a', 'que', 'nos', 'pongamos', 'de', 'pie', 'para', 'guardar', 'un', 'minuto', 'de', 'silencio', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8ebc99"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully tokenized the Spanish sentences. Now I need to build a vocabulary from these tokenized Spanish sentences, similar to how the English vocabulary was built. This is the next logical step in preparing the Spanish data for the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b9dc0e3",
        "outputId": "6eb7763f-381e-48f1-a6c4-e35147528753"
      },
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# Build vocabulary for Spanish with special tokens\n",
        "# UNK_IDX and PAD_IDX are already defined globally from the English vocabulary creation\n",
        "# UNK_IDX = 0\n",
        "# PAD_IDX = 1\n",
        "\n",
        "vocab_es = build_vocab_from_iterator(\n",
        "    [sentence for sentence in tokenized_spanish_sentences], # Pass an iterator of token lists\n",
        "    min_freq=1,\n",
        "    specials=['<unk>', '<pad>', '<bos>', '<eos>'],\n",
        "    special_first=True\n",
        ")\n",
        "vocab_es.set_default_index(UNK_IDX)\n",
        "\n",
        "print(f\"Spanish Vocabulary size: {len(vocab_es)}\")\n",
        "print(f\"Example mapping: 'hola' -> {vocab_es(['hola'])[0]}, '<pad>' -> {vocab_es(['<pad>'])[0]}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spanish Vocabulary size: 66909\n",
            "Example mapping: 'hola' -> 0, '<pad>' -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180bd507"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Spanish vocabulary has been built, the next step is to numericalize the tokenized Spanish sentences (convert tokens to their vocabulary IDs) and then pad them to a consistent length, similar to how the English sentences were processed. This will prepare the Spanish data for model input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86e07c14",
        "outputId": "18aca0ee-52b0-4e85-a125-8341643f9604"
      },
      "source": [
        "MAX_SEQ_LEN = 100 # Ensure MAX_SEQ_LEN is defined or accessible\n",
        "\n",
        "# Numericalize and pad the Spanish sentences\n",
        "output_sequences_numerical = numericalize_and_pad(tokenized_spanish_sentences, vocab_es, MAX_SEQ_LEN, PAD_IDX)\n",
        "\n",
        "print(f\"Shape of numerical output sequences: {output_sequences_numerical.shape}\")\n",
        "print(\"First numerical output sequence:\", output_sequences_numerical[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of numerical output sequences: torch.Size([149559, 100])\n",
            "First numerical output sequence: tensor([   2, 5649, 8464,   10,  572,    4, 1219,   16,   37,   82,    5, 5005,\n",
            "          10, 2013, 2062,    4, 1082,  280,    5,   11, 9664,   12, 2202,  273,\n",
            "          75,  607,    4,    7,  830,  491,  434, 1362, 5570,    8,    3,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b603cf1"
      },
      "source": [
        "## Define Decoder Block\n",
        "\n",
        "### Subtask:\n",
        "Define the `DecoderBlock` module. This block will incorporate masked multi-head self-attention (for the target sequence), multi-head attention over the encoder's output, a feed-forward network, residual connections, and layer normalization, following the standard Transformer architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a49f5ea"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `DecoderBlock` class as instructed, incorporating masked multi-head self-attention, multi-head attention over the encoder's output, a feed-forward network, residual connections, layer normalization, and dropout layers, following the provided structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "361dd098",
        "outputId": "88ea4a59-d934-4565-de88-70f86c5d688b"
      },
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        # Masked Multi-Head Self-Attention for the target sequence\n",
        "        self.self_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # Multi-Head Attention over the encoder's output\n",
        "        self.encoder_attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = FeedForwardBlock(embed_dim, forward_expansion)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, trg_mask, src_mask):\n",
        "        # Masked Multi-Head Self-Attention\n",
        "        # Query, Key, Value are all from the target sequence (x)\n",
        "        self_attention = self.self_attention(x, x, x, trg_mask)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        x = self.dropout1(self.norm1(self_attention + x))\n",
        "\n",
        "        # Multi-Head Encoder-Decoder Attention\n",
        "        # Query from target sequence (x), Key and Value from encoder output (enc_output)\n",
        "        encoder_attention = self.encoder_attention(enc_output, enc_output, x, src_mask)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        x = self.dropout2(self.norm2(encoder_attention + x))\n",
        "\n",
        "        # Feed-forward network\n",
        "        feed_forward = self.feed_forward(x)\n",
        "        # Add residual connection, apply dropout and layer normalization\n",
        "        out = self.dropout3(self.norm3(feed_forward + x))\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"DecoderBlock class defined.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderBlock class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b282c71"
      },
      "source": [
        "**Reasoning**:\n",
        "The `DecoderBlock` class has been defined. Now, I need to test its functionality by creating a dummy input, `enc_output`, and the necessary masks, then instantiating the `DecoderBlock` and passing these through its `forward` method. This will verify that the block can process inputs and produce an output with the expected shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71e3319e",
        "outputId": "619f8032-cbb0-49e7-c680-6e44eeebee88"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Assuming embed_dim, num_heads, forward_expansion, dropout are already defined globally\n",
        "# EMBEDDING_DIM = 256\n",
        "# NUM_HEADS = 8\n",
        "# FORWARD_EXPANSION = 4\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "# Example Usage:\n",
        "N = 2 # Batch size\n",
        "seq_len_trg = 15 # Target sequence length (for x)\n",
        "seq_len_src = MAX_SEQ_LEN # Source sequence length (for enc_output, from MAX_SEQ_LEN)\n",
        "\n",
        "# Dummy input for the decoder (target embeddings)\n",
        "x = torch.randn(N, seq_len_trg, EMBEDDING_DIM)\n",
        "\n",
        "# Dummy encoder output\n",
        "enc_output = torch.randn(N, seq_len_src, EMBEDDING_DIM)\n",
        "\n",
        "# Dummy target mask (for masked self-attention in decoder)\n",
        "# This mask typically ensures that a token cannot attend to future tokens.\n",
        "# For simplicity, let's use a causal mask. (seq_len_trg, seq_len_trg)\n",
        "trg_mask = torch.tril(torch.ones(seq_len_trg, seq_len_trg)).unsqueeze(0).unsqueeze(0).expand(N, NUM_HEADS, seq_len_trg, seq_len_trg)\n",
        "\n",
        "# Dummy source mask (for encoder-decoder attention)\n",
        "# This mask typically handles padding in the encoder output. Here, assuming no padding mask needed.\n",
        "# For simplicity, we can use an all-ones mask if no actual padding mask is being generated.\n",
        "src_mask = torch.ones(N, 1, 1, seq_len_src) # N, 1, 1, src_seq_len for broadcasting\n",
        "\n",
        "# Instantiate the DecoderBlock\n",
        "decoder_block = DecoderBlock(EMBEDDING_DIM, NUM_HEADS, FORWARD_EXPANSION, DROPOUT)\n",
        "\n",
        "# Pass the inputs through the DecoderBlock\n",
        "output_decoder_block = decoder_block(x, enc_output, trg_mask, src_mask)\n",
        "\n",
        "print(f\"Shape of output after DecoderBlock: {output_decoder_block.shape}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of output after DecoderBlock: torch.Size([2, 15, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b90f172"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the successful implementation and testing of the Decoder Block and prepare for integrating it into a full Transformer model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e23254a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Spanish sentence tokenization was successfully performed using spaCy's `es_core_news_sm` model, processing 149,559 sentences.\n",
        "*   A Spanish vocabulary was constructed, comprising 66,909 unique tokens, including special tokens like `<unk>`, `<pad>`, `<bos>`, and `<eos>`.\n",
        "*   The tokenized Spanish sentences were numericalized and padded to a uniform length of 100, resulting in a tensor of shape `(149559, 100)`.\n",
        "*   The `DecoderBlock` module was successfully defined, incorporating key Transformer components: masked multi-head self-attention, encoder-decoder attention, a feed-forward network, residual connections, and layer normalization.\n",
        "*   The functionality of the `DecoderBlock` was validated with dummy inputs, producing an output of shape `(2, 15, 256)`, which aligns with expected dimensions for a batch size of 2, a target sequence length of 15, and an embedding dimension of 256.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The successfully tokenized, vocabulary-built, numericalized, and padded Spanish dataset is now ready for use as target sequences in a sequence-to-sequence model.\n",
        "*   The validated `DecoderBlock` can now be integrated into a complete Transformer Decoder, and subsequently into a full Transformer model for machine translation or other sequence generation tasks.\n"
      ]
    }
  ]
}