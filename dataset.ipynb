{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9IisqicXc62lDa7/XJzzB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charchar1245/Transformer-for-Translating-English-to-Spanish/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "G2Mtx2RCt6jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKjYwofds4ww",
        "outputId": "ee8f4a3a-a287-4871-be82-ac7054e99e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Import dataset from Huggingface\n",
        "ds = load_dataset(\"okezieowen/english_to_spanish\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing purposes in this file, we will just take the first 10 sentences from the data\n",
        "selected_data = ds['train'].select(range(10))\n",
        "df = selected_data.to_pandas()"
      ],
      "metadata": {
        "id": "5JUt1ZuHs_ri"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Tokenize text\n",
        "def tokenize_en(text):\n",
        "  spacy_en = spacy.load(\"en_core_web_sm\")\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "# New variable for tokenized text\n",
        "all_tokens = [tokenize_en(sentence) for sentence in df['English'] if sentence is not None]\n"
      ],
      "metadata": {
        "id": "hsZjtzdzujON"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in all_tokens:\n",
        "  print(tokens)"
      ],
      "metadata": {
        "id": "V6FHs_Ny63yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchtext\n",
        "!pip install torch==2.2.2 torchtext==0.17.2"
      ],
      "metadata": {
        "id": "Xu1AwyK87FFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    all_tokens,\n",
        "    min_freq=1,\n",
        "    specials=[\"<unk>\", \"<pad>\"],\n",
        "    special_first=True\n",
        ")\n",
        "\n",
        "print(vocab.get_itos())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Afnj2BlAtcy",
        "outputId": "24538d6d-18ff-4e41-f4a1-c7d435e6c20a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', 'the', ',', 'a', 'of', '.', 'to', 'in', 'and', \"'\", 'Sri', 'have', 'on', 'you', 'European', 'I', 'Lanka', 'Parliament', 'President', 'minute', 'number', 's', 'silence', 'that', 'this', 'Madam', 'You', 'as', 'be', 'countries', 'few', 'for', 'like', 'people', 'requested', 'session', 'very', 'will', \"'s\", '(', ')', '-', '17', '1999', '?', 'Although', 'December', 'Friday', 'House', 'In', 'Kumar', 'Lankan', 'Members', 'Mr', 'One', 'Please', 'Ponnambalam', 'The', 'Union', 'Would', 'adjourned', 'again', 'ago', 'all', 'appropriate', 'assassinated', 'at', 'aware', 'been', 'behalf', 'bomb', 'bug', 'can', 'concerned', 'course', 'days', 'deaths', 'debate', 'declare', 'difficult', 'disasters', 'do', 'dreaded', 'dreadful', 'during', 'enjoyed', 'everything', 'explosions', 'expressing', 'failed', 'festive', 'from', 'had', 'happy', 'her', 'his', 'hope', 'it', 'just', 'killings', 'letter', 'materialise', 'meantime', 'millennium', 'months', 'natural', 'new', 'next', 'observe', 'observed', 'once', 'order', 'other', 'part', 'particularly', 'peaceful', 'period', 'pleasant', 'point', 'possibly', 'press', 'recently', 'reconciliation', 'regret', 'resumed', 'rise', 'rose', 'seek', 'seen', 'series', 'she', 'should', 'situation', 'still', 'storms', 'subject', 'suffered', 'television', 'terrible', 'then', 'there', 'those', 'truly', 'urging', 'various', 'victims', 'violent', 'visited', 'was', 'were', 'who', 'wish', 'would', 'write', 'year']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numericalize the all_tokens array\n",
        "def numericalize(list_of_token_lists):\n",
        "  return [vocab(tokens) for tokens in list_of_token_lists]\n",
        "\n",
        "\n",
        "print(numericalize(all_tokens))\n"
      ],
      "metadata": {
        "id": "IMTCHcL1JI95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}